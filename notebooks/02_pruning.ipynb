{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6172af",
   "metadata": {},
   "source": [
    "# 2. Pruning\n",
    "\n",
    "This notebook demonstrates how to prune a model using the `torh.torch.nn.utils.prune` and `torch-pruning` library. Pruning is a technique to reduce the size of a neural network by removing weights that are deemed unnecessary, which can lead to faster inference times and reduced memory usage.\n",
    "\n",
    "There is 2 types of pruning:\n",
    "- **Unstructured pruning**: Removes individual weights using an importance metric (e.g., low-magnitude weights are pruned). This can lead to sparse models, which reduce drastically the number of parameters but must rely on specialized hardware and/or libraries to take advantage of the sparsity during inference.\n",
    "- **Structured pruning**: Removes entire channels or layers, using a metric measuring an entire channel or layer importance (e.g., low-magnitude channels are pruned). This leads to a more regular model that can be used on standard hardware without requiring specialized libraries.\n",
    "\n",
    "Metrics used for pruning are typically based on the magnitude of weights, gradients, or other statistics that indicate the importance of a weight or a channel.\n",
    "\n",
    "The process is defined as such:\n",
    "* A Torch model is loaded.\n",
    "* A pruning strategy is defined, which specifies how to prune the model (e.g., unstructured or structured pruning, and the importance metric to use).\n",
    "* The model is pruned using the defined strategy.\n",
    "* The model is exported PyTorch format for further optimization or deployment.\n",
    "\n",
    "2 pruning methods will be used in this notebook, both for 2 models (image and audio classification):\n",
    "* L1-magntiude unstructured pruning using `torch.torch.nn.utils.prune`.\n",
    "* L1-magnitude structured pruning using `torch-pruning`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405ace3",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b5e6b",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "498b4ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pynvml available: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Callable, Literal\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torch.optim import Optimizer\n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.nn.utils.prune as unstruct_prune\n",
    "import torch_pruning as struct_prune\n",
    "\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import psutil\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    pynvml_available = True\n",
    "except (ImportError, pynvml.NVMLError):\n",
    "    pynvml_available = False\n",
    "print(f\"pynvml available: {pynvml_available}\")\n",
    "\n",
    "# Setup device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "DTYPE = torch.bfloat16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "# Base directories for datasets and models\n",
    "BASE_DATA_DIR = \"../data\"\n",
    "IMAGE_DATA_DIR = os.path.join(BASE_DATA_DIR, \"image\")\n",
    "BASE_MODEL_DIR = \"../models\"\n",
    "MODEL_BASELINE_DIR = os.path.join(BASE_MODEL_DIR, \"baseline\")\n",
    "\n",
    "# CIFAR-10 dataset directories\n",
    "CIFAR10_DIR = os.path.join(IMAGE_DATA_DIR, \"cifar10\")\n",
    "CIFAR10_TRAIN_DIR = os.path.join(CIFAR10_DIR, \"train\")\n",
    "CIFAR10_TRAIN_PT_FILE = os.path.join(CIFAR10_TRAIN_DIR, \"data.pt\")\n",
    "CIFAR10_VAL_DIR = os.path.join(CIFAR10_DIR, \"val\")\n",
    "CIFAR10_VAL_PT_FILE = os.path.join(CIFAR10_VAL_DIR, \"data.pt\")\n",
    "CIFAR10_TEST_DIR = os.path.join(CIFAR10_DIR, \"test\")\n",
    "CIFAR10_TEST_PT_FILE = os.path.join(CIFAR10_TEST_DIR, \"data.pt\")\n",
    "\n",
    "# MobileNetV2 model directory\n",
    "MOBILENETV2_CIFAR10_BASELINE_PT_FILE = os.path.join(MODEL_BASELINE_DIR, \"mobilenetv2_cifar10.pt\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler()])\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b508f",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "127fa4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 10:59:04,076 - __main__ - INFO - MobileNetV2 model for CIFAR-10 loaded from state_dict and prepared on cuda.\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate the base MobileNetV2 architecture\n",
    "# We use weights=None because we will load our own fine-tuned weights.\n",
    "mobilnet_v2_cifar10_base = torchvision.models.mobilenet_v2(weights=None)\n",
    "num_classes_cifar10 = 10\n",
    "\n",
    "# 2. Adapt the classifier head to match the CIFAR-10 adaptation (10 classes)\n",
    "# This is necessary so the architecture matches the saved state_dict.\n",
    "if hasattr(mobilnet_v2_cifar10_base, 'classifier') and isinstance(mobilnet_v2_cifar10_base.classifier, torch.nn.Sequential):\n",
    "    if hasattr(mobilnet_v2_cifar10_base.classifier[-1], 'in_features'):\n",
    "        in_features = mobilnet_v2_cifar10_base.classifier[-1].in_features\n",
    "        mobilnet_v2_cifar10_base.classifier[-1] = torch.nn.Linear(in_features, num_classes_cifar10)\n",
    "    else:\n",
    "        # This case should ideally not be hit if MobileNetV2 structure is standard\n",
    "        logger.error(\"Could not find 'in_features' in the last layer of the classifier to adapt.\")\n",
    "        raise AttributeError(\"Could not find 'in_features' in the last layer of the classifier.\")\n",
    "elif hasattr(mobilnet_v2_cifar10_base, 'fc'): # Fallback for models using 'fc'\n",
    "     in_features = mobilnet_v2_cifar10_base.fc.in_features\n",
    "     mobilnet_v2_cifar10_base.fc = torch.nn.Linear(in_features, num_classes_cifar10)\n",
    "else:\n",
    "    logger.error(\"Model does not have a known 'classifier' (Sequential) or 'fc' (Linear) attribute to adapt.\")\n",
    "    raise AttributeError(\"Model does not have a known classifier structure to adapt.\")\n",
    "\n",
    "# 3. Load the saved state_dict\n",
    "# The MOBILENETV2_CIFAR10_BASELINE_PT_FILE contains the state_dict.\n",
    "saved_state_dict = torch.load(MOBILENETV2_CIFAR10_BASELINE_PT_FILE, map_location=DEVICE)\n",
    "mobilnet_v2_cifar10_base.load_state_dict(saved_state_dict)\n",
    "\n",
    "# 4. Move model to the correct device and set to evaluation mode\n",
    "mobilnet_v2_cifar10_base.to(DEVICE)\n",
    "mobilnet_v2_cifar10_base.eval() # Important if you're not immediately training\n",
    "\n",
    "logger.info(f\"MobileNetV2 model for CIFAR-10 loaded from state_dict and prepared on {DEVICE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ac093",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7a205",
   "metadata": {},
   "source": [
    "### L1 magnitude unstructured pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92ea294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_weights_with_l1_unstructured_pruning(model: torch.nn.Module, \n",
    "                                  pruning_amount: float, \n",
    "                                  layers_to_prune: tuple = (torch.nn.Linear, torch.nn.Conv2d),\n",
    "                                  parameter_name: str = \"weight\") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Applies L1 unstructured pruning to specified layers of a model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to prune.\n",
    "        pruning_amount (float): The fraction of connections to prune (e.g., 0.2 for 20%).\n",
    "        layers_to_prune (tuple): A tuple of layer types to prune (e.g., (torch.nn.Linear, torch.nn.Conv2d)).\n",
    "        parameter_name (str): The name of the parameter to prune within the layers (e.g., \"weight\", \"bias\").\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The model with pruning applied (reparameterized).\n",
    "    \"\"\"\n",
    "    if not (0.0 < pruning_amount < 1.0):\n",
    "        raise ValueError(\"Pruning amount must be between 0.0 and 1.0 (exclusive).\")\n",
    "\n",
    "    logger.info(f\"Applying L1 unstructured pruning with amount: {pruning_amount:.2f} for parameter '{parameter_name}' in layers: {[layer.__name__ for layer in layers_to_prune]}\")\n",
    "    num_pruned_layers = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, layers_to_prune):\n",
    "            try:\n",
    "                unstruct_prune.l1_unstructured(module, name=parameter_name, amount=pruning_amount)\n",
    "                logger.debug(f\"Pruned {parameter_name} of layer: {module}\")\n",
    "                num_pruned_layers +=1\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not prune {parameter_name} of layer {module}: {e}\")\n",
    "    \n",
    "    if num_pruned_layers == 0:\n",
    "        logger.warning(\"No layers were pruned. Check 'layers_to_prune' and model structure.\")\n",
    "    else:\n",
    "        logger.info(f\"Applied L1 unstructured pruning to {num_pruned_layers} layers.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def remove_pruning_reparameterization(model: torch.nn.Module,\n",
    "                                      layers_to_prune: tuple = (torch.nn.Linear, torch.nn.Conv2d),\n",
    "                                      parameter_name: str = \"weight\") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Removes the pruning reparameterization, making the pruning permanent.\n",
    "    The pruned weights are set to zero directly in the parameter tensor.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model with pruning applied.\n",
    "        layers_to_prune (tuple): A tuple of layer types from which to remove reparameterization.\n",
    "        parameter_name (str): The name of the parameter that was pruned.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The model with pruning made permanent.\n",
    "\n",
    "    Notes:\n",
    "        Must be called after `mark_weights_with_l1_unstructured_pruning`.\n",
    "    \"\"\"\n",
    "    logger.info(\"Making pruning permanent by removing reparameterization...\")\n",
    "    num_permanent_layers = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, layers_to_prune):\n",
    "            if unstruct_prune.is_pruned(module): # Check if the module has pruning hooks\n",
    "                try:\n",
    "                    unstruct_prune.remove(module, name=parameter_name)\n",
    "                    logger.debug(f\"Made pruning permanent for {parameter_name} of layer: {module}\")\n",
    "                    num_permanent_layers +=1\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not make pruning permanent for {parameter_name} of layer {module}: {e}\")\n",
    "            \n",
    "    if num_permanent_layers == 0:\n",
    "        logger.warning(\"No pruning reparameterization was removed. Was the model pruned?\")\n",
    "    else:\n",
    "        logger.info(f\"Made pruning permanent for {num_permanent_layers} layers.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_sparsity(model: torch.nn.Module, \n",
    "                       layers_to_check: tuple = (torch.nn.Linear, torch.nn.Conv2d),\n",
    "                       parameter_name: str = \"weight\") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates the sparsity of specified parameters in the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to check.\n",
    "        layers_to_check (tuple): Layer types to inspect.\n",
    "        parameter_name (str): Name of the parameter (e.g., \"weight\").\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing overall sparsity and sparsity per layer.\n",
    "\n",
    "    Notes:\n",
    "        Must be applied after `mark_weights_with_l1_unstructured_pruning` and `remove_pruning_reparameterization`.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    total_zeros = 0\n",
    "    total_elements = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, layers_to_check):\n",
    "            if hasattr(module, parameter_name):\n",
    "                param = getattr(module, parameter_name)\n",
    "                if param is not None:\n",
    "                    # If pruning is applied but not made permanent, the original tensor might not be zero.\n",
    "                    # We need to check the 'weight_orig' and 'weight_mask' if they exist.\n",
    "                    # However, if remove_pruning_reparameterization has been called,\n",
    "                    # the 'weight' tensor itself will contain zeros.\n",
    "                    \n",
    "                    # For simplicity after `remove`, we check the actual parameter.\n",
    "                    # If `remove` hasn't been called, this will report sparsity of the\n",
    "                    # underlying tensor before the mask is applied during forward pass.\n",
    "                    # For true sparsity *during* forward pass before `remove`, one would\n",
    "                    # need to access module.weight_mask and module.weight_orig.\n",
    "                    \n",
    "                    layer_zeros = torch.sum(param.data == 0).item()\n",
    "                    layer_elements = param.data.numel()\n",
    "                    total_zeros += layer_zeros\n",
    "                    total_elements += layer_elements\n",
    "                    if layer_elements > 0:\n",
    "                        layer_sparsity = layer_zeros / layer_elements\n",
    "                        results[f\"{name}.{parameter_name}_sparsity\"] = layer_sparsity\n",
    "                        logger.debug(f\"Sparsity of {name}.{parameter_name}: {layer_sparsity:.4f} ({layer_zeros}/{layer_elements})\")\n",
    "                    else:\n",
    "                        results[f\"{name}.{parameter_name}_sparsity\"] = 0.0\n",
    "                        logger.debug(f\"Layer {name}.{parameter_name} has 0 elements.\")\n",
    "\n",
    "\n",
    "    overall_sparsity = total_zeros / total_elements if total_elements > 0 else 0.0\n",
    "    results[\"overall_sparsity\"] = overall_sparsity\n",
    "    logger.info(f\"Overall sparsity ({parameter_name}): {overall_sparsity:.4f} ({total_zeros}/{total_elements})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def apply_l1_unstructured_pruning(model: torch.nn.Module, \n",
    "                                  pruning_amount: float, \n",
    "                                  layers_to_prune: tuple = (torch.nn.Linear, torch.nn.Conv2d),\n",
    "                                  parameter_name: str = \"weight\") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Applies L1 unstructured pruning to the model and returns the pruned model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to prune.\n",
    "        pruning_amount (float): The fraction of connections to prune.\n",
    "        layers_to_prune (tuple): Layers to apply pruning to.\n",
    "        parameter_name (str): The name of the parameter to prune.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The pruned model.\n",
    "    \"\"\"\n",
    "    model = mark_weights_with_l1_unstructured_pruning(model, pruning_amount, layers_to_prune, parameter_name)\n",
    "    model = remove_pruning_reparameterization(model, layers_to_prune, parameter_name)\n",
    "    logger.info(\"L1 unstructured pruning completed.\")\n",
    "    _ = calculate_sparsity(model, layers_to_prune, parameter_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4dca5",
   "metadata": {},
   "source": [
    "### L1 magnitude structured pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34916b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_parameters(model: torch.nn.Module, only_trainable: bool = True) -> int:\n",
    "    \"\"\"Counts the total number of parameters in a model.\"\"\"\n",
    "    if only_trainable:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def apply_l1_structured_pruning(\n",
    "    model: torch.nn.Module,\n",
    "    example_inputs: torch.Tensor,\n",
    "    pruning_amount: float,\n",
    "    layers_to_prune: tuple[type, ...] = (torch.nn.Conv2d, torch.nn.Linear),\n",
    "    ignored_layers: list[torch.nn.Module] | None = None,\n",
    "    prune_output_channels: bool = True\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Applies L1 magnitude structured pruning to specified layers of a model\n",
    "    by removing a fraction of channels/features from each targeted layer.\n",
    "\n",
    "    By default, it prunes output channels of Conv2d layers and output features\n",
    "    of Linear layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to prune.\n",
    "        example_inputs (torch.Tensor): A batch of example inputs for dependency tracing.\n",
    "                                       Should be on the same device as the model.\n",
    "        pruning_amount (float): The fraction of channels/features to prune from each\n",
    "                                targeted layer (e.g., 0.2 for 20%).\n",
    "        layers_to_prune (tuple[type, ...]): Tuple of layer types to prune.\n",
    "        ignored_layers (list[torch.nn.Module] | None): A list of specific layer\n",
    "                                                    modules to ignore during pruning.\n",
    "        prune_output_channels (bool): If True, prunes output channels/features.\n",
    "                                      If False, attempts to prune input channels/features\n",
    "                                      (Note: Input channel importance calculation here is simplified).\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The model with structured pruning applied. The model is modified in-place.\n",
    "    \"\"\"\n",
    "    if not (0.0 <= pruning_amount < 1.0):\n",
    "        raise ValueError(\"Pruning amount must be between 0.0 (inclusive) and 1.0 (exclusive).\")\n",
    "\n",
    "    if pruning_amount == 0.0:\n",
    "        logger.info(\"Pruning amount is 0.0. No structured pruning will be applied.\")\n",
    "        return model\n",
    "\n",
    "    # torch-pruning usually expects the model in eval mode for graph construction\n",
    "    original_mode_is_train = model.training\n",
    "    model.eval()\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    example_inputs = example_inputs.to(device)\n",
    "\n",
    "    logger.info(f\"Applying L1 structured pruning with amount: {pruning_amount:.2f}\")\n",
    "    initial_params = count_model_parameters(model)\n",
    "    logger.info(f\"Initial model parameters: {initial_params}\")\n",
    "\n",
    "    DG = struct_prune.DependencyGraph()\n",
    "    DG.build_dependency(model, example_inputs=example_inputs)\n",
    "\n",
    "    num_pruned_overall_layers = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, layers_to_prune):\n",
    "            if ignored_layers and module in ignored_layers:\n",
    "                logger.debug(f\"Skipping ignored layer: {name} ({type(module).__name__})\")\n",
    "                continue\n",
    "\n",
    "            current_channels = 0\n",
    "            pruning_fn = None\n",
    "            dim_type = \"\"\n",
    "            weights = module.weight.data\n",
    "\n",
    "            if prune_output_channels:\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    current_channels = module.out_channels\n",
    "                    pruning_fn = struct_prune.prune_conv_out_channels\n",
    "                    dim_type = \"output channels\"\n",
    "                    # L1 norm for each output filter: (C_out, C_in, K_h, K_w) -> sum over C_in, K_h, K_w\n",
    "                    channel_importance = torch.norm(weights.flatten(1), p=1, dim=1)\n",
    "                elif isinstance(module, torch.nn.Linear):\n",
    "                    current_channels = module.out_features\n",
    "                    pruning_fn = struct_prune.prune_linear_out_features\n",
    "                    dim_type = \"output features\"\n",
    "                    # L1 norm for each output feature's weights: (F_out, F_in) -> sum over F_in\n",
    "                    channel_importance = torch.norm(weights, p=1, dim=1)\n",
    "                else: # Should not be reached if layers_to_prune is respected\n",
    "                    continue\n",
    "            else: # Pruning input channels\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    current_channels = module.in_channels\n",
    "                    pruning_fn = struct_prune.prune_conv_in_channels\n",
    "                    dim_type = \"input channels\"\n",
    "                    # Simplified L1 for input channels: (C_out, C_in, K_h, K_w) -> transpose to (C_in, C_out, K_h, K_w)\n",
    "                    channel_importance = torch.norm(weights.transpose(0,1).contiguous().flatten(1), p=1, dim=1)\n",
    "                elif isinstance(module, torch.nn.Linear):\n",
    "                    current_channels = module.in_features\n",
    "                    pruning_fn = struct_prune.prune_linear_in_features\n",
    "                    dim_type = \"input features\"\n",
    "                    # Simplified L1 for input features: (F_out, F_in) -> transpose to (F_in, F_out)\n",
    "                    channel_importance = torch.norm(weights.T.contiguous(), p=1, dim=1)\n",
    "                else: # Should not be reached\n",
    "                    continue\n",
    "\n",
    "            if current_channels == 0:\n",
    "                logger.warning(f\"Layer {name} ({type(module).__name__}) has 0 {dim_type} to prune. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            num_to_prune = int(pruning_amount * current_channels)\n",
    "\n",
    "            if num_to_prune == 0:\n",
    "                logger.debug(f\"Layer {name}: No {dim_type} to prune with amount {pruning_amount:.2f} (Total: {current_channels}).\")\n",
    "                continue\n",
    "\n",
    "            # Ensure we don't prune all channels, as torch-pruning might error or lead to a dead network.\n",
    "            # It's safer to leave at least one channel.\n",
    "            if num_to_prune >= current_channels:\n",
    "                num_to_prune = current_channels - 1\n",
    "                logger.warning(\n",
    "                    f\"Layer {name}: Pruning amount {pruning_amount:.2f} would remove all or too many {dim_type}. \"\n",
    "                    f\"Adjusting to prune {num_to_prune} {dim_type} to keep at least one.\"\n",
    "                )\n",
    "                if num_to_prune <= 0: # If current_channels was 1\n",
    "                    logger.info(f\"Layer {name}: Cannot prune, only 1 {dim_type} exists. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "            # Get indices of channels to prune (those with smallest L1 norm)\n",
    "            sorted_channel_indices = torch.argsort(channel_importance)\n",
    "            pruning_indices = sorted_channel_indices[:num_to_prune].tolist()\n",
    "\n",
    "            try:\n",
    "                pruning_plan = DG.get_pruning_plan(module, pruning_fn, idxs=pruning_indices)\n",
    "                if pruning_plan:\n",
    "                    logger.debug(f\"Pruning {num_to_prune} {dim_type} from layer {name} ({type(module).__name__}). Smallest L1 norm indices (first 10): {pruning_indices[:10]}...\")\n",
    "                    pruning_plan.exec()\n",
    "                    num_pruned_overall_layers += 1\n",
    "                else:\n",
    "                    logger.warning(f\"Could not generate pruning plan for layer {name} ({type(module).__name__}).\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to prune layer {name} ({type(module).__name__}): {e}\", exc_info=True)\n",
    "\n",
    "    if num_pruned_overall_layers == 0:\n",
    "        logger.warning(\"No layers were structurally pruned. Check 'layers_to_prune', model structure, and pruning_amount.\")\n",
    "    else:\n",
    "        logger.info(f\"Applied structured pruning to {num_pruned_overall_layers} layers.\")\n",
    "\n",
    "    final_params = count_model_parameters(model)\n",
    "    logger.info(f\"Final model parameters after structured pruning: {final_params}\")\n",
    "    if initial_params > 0 :\n",
    "        reduction_percent = (initial_params - final_params) / initial_params * 100\n",
    "        logger.info(f\"Parameter reduction: {initial_params - final_params} ({reduction_percent:.2f}%)\")\n",
    "    else:\n",
    "        logger.info(f\"Parameter reduction: {initial_params - final_params}\")\n",
    "\n",
    "\n",
    "    if original_mode_is_train:\n",
    "        model.train() # Set back to original mode\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0356a",
   "metadata": {},
   "source": [
    "# L1 unstructured pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53db271",
   "metadata": {},
   "source": [
    "## One-step pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efe872f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 11:00:39,356 - __main__ - INFO - Applying L1 unstructured pruning with amount: 0.20 for parameter 'weight' in layers: ['Linear', 'Conv2d']\n",
      "2025-06-10 11:00:39,397 - __main__ - INFO - Applied L1 unstructured pruning to 53 layers.\n",
      "2025-06-10 11:00:39,398 - __main__ - INFO - Making pruning permanent by removing reparameterization...\n",
      "2025-06-10 11:00:39,400 - __main__ - INFO - Made pruning permanent for 53 layers.\n",
      "2025-06-10 11:00:39,401 - __main__ - INFO - L1 unstructured pruning completed.\n",
      "2025-06-10 11:00:39,411 - __main__ - INFO - Overall sparsity (weight): 0.2000 (440512/2202560)\n"
     ]
    }
   ],
   "source": [
    "# Apply L1 unstructured pruning to the MobileNetV2 model\n",
    "mobilnet_v2_cifar10_unstruct_prune = apply_l1_unstructured_pruning(mobilnet_v2_cifar10_base, pruning_amount=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293e56e",
   "metadata": {},
   "source": [
    "## Iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216f5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b64bcd3",
   "metadata": {},
   "source": [
    "# L1 structured pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9aab16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
