{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d23731",
   "metadata": {},
   "source": [
    "# 4. Exporting Models to ONNX and Running Inference\n",
    "\n",
    "This notebook demonstrates how to export the trained/optimized PyTorch models to the ONNX (Open Neural Network Exchange) format. It also shows how to run inference using ONNX Runtime on the exported models.\n",
    "\n",
    "We will export three specific model versions:\n",
    "1.  **Baseline FP32 Model**: The original MobileNetV2 model adapted for CIFAR-10.\n",
    "3.  **Baseline QAT INT8 Model**: The baseline model quantized to INT8 using Quantization-Aware Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b7098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbeuran/repos/nnopt/.venv/lib/python3.12/site-packages/openvino/runtime/__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n",
      "2025-06-13 16:07:16,949 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Using device: cuda, dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "ONNX Runtime version: 1.22.0\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "from nnopt.model.export import export_model_to_onnx\n",
    "from nnopt.recipes.mobilenetv2_cifar10 import load_mobilenetv2_cifar10_model\n",
    "from nnopt.model.prune import remove_pruning_reparameterization\n",
    "from nnopt.model.const import BASE_MODEL_DIR, DEVICE\n",
    "\n",
    "# Ensure the logger in export is configured (if not already by its import)\n",
    "import logging\n",
    "logger = logging.getLogger(\"nnopt.model.export\")\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "                        handlers=[logging.StreamHandler()])\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1915f",
   "metadata": {},
   "source": [
    "## Configuration and Dummy Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5d8004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX models will be saved in: /home/pbeuran/repos/nnopt/models/onnx_exports\n",
      "Dummy input shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Define model versions to be exported\n",
    "baseline_fp32_version = \"mobilenetv2_cifar10/fp32/baseline\"\n",
    "struct_pruned_fp32_version = \"mobilenetv2_cifar10/fp32/l1_struct_prune_0.3\"\n",
    "pqt_int8_version = \"mobilenetv2_cifar10/int8/pqt_baseline\"\n",
    "qat_int8_version = \"mobilenetv2_cifar10/int8/qat_baseline\"\n",
    "unstruct_pruned_fp32_version = \"mobilenetv2_cifar10/fp32/l1_unstruct_prune_0.7\"\n",
    "pqt_int8_unstruct_pruned_version = \"mobilenetv2_cifar10/int8/pqt_l1_unstruct_prune_0.7\"\n",
    "qat_int8_unstruct_pruned_version = \"mobilenetv2_cifar10/int8/qat_l1_unstruct_prune_0.7\"\n",
    "\n",
    "\n",
    "# Directory to save ONNX models\n",
    "ONNX_EXPORT_DIR = os.path.join(BASE_MODEL_DIR, \"onnx_exports\")\n",
    "os.makedirs(ONNX_EXPORT_DIR, exist_ok=True)\n",
    "print(f\"ONNX models will be saved in: {ONNX_EXPORT_DIR}\")\n",
    "\n",
    "# Create a dummy input tensor (batch_size, channels, height, width)\n",
    "# MobileNetV2 typically expects 224x224 images.\n",
    "# CIFAR-10 images are 32x32, but the model adapts them or uses a standard input size.\n",
    "# Using 224x224 as per common MobileNetV2 usage and example in pruning notebook.\n",
    "dummy_input_shape = (1, 3, 224, 224)\n",
    "dummy_input = torch.randn(dummy_input_shape, device='cpu') # ONNX export prefers CPU dummy input\n",
    "print(f\"Dummy input shape: {dummy_input.shape}\")\n",
    "\n",
    "# Define dynamic axes for batch size flexibility\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdfd51d",
   "metadata": {},
   "source": [
    "## 1. Baseline FP32 Model Export & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dbab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:17,825 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading MobileNetV2 model for CIFAR-10 from version: mobilenetv2_cifar10/fp32/baseline at /home/pbeuran/repos/nnopt/models\n",
      "2025-06-13 16:07:17,826 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loaded metadata: {'metrics_values': {'val_metrics': {'accuracy': 0.9254, 'avg_loss': 0.21455251355171204, 'samples_per_second': 9594.238581494608, 'avg_time_per_batch': 0.006596786050597328, 'avg_time_per_sample': 0.00010422921959943779, 'params_stats': {'int_weight_params': 0, 'float_weight_params': 2202560, 'float_bias_params': 10, 'bn_param_params': 34112, 'other_float_params': 0, 'total_params': 2236682, 'approx_memory_mb_for_params': 8.532264709472656}}, 'test_metrics': {'accuracy': 0.9288, 'avg_loss': 0.20640371625423432, 'samples_per_second': 9116.575566348814, 'avg_time_per_batch': 0.006986643948966146, 'avg_time_per_sample': 0.00010969030999876849, 'params_stats': {'int_weight_params': 0, 'float_weight_params': 2202560, 'float_bias_params': 10, 'bn_param_params': 34112, 'other_float_params': 0, 'total_params': 2236682, 'approx_memory_mb_for_params': 8.532264709472656}}}}\n",
      "2025-06-13 16:07:17,827 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading JIT traced model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/fp32/baseline/jit_trace.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline FP32 model from version: mobilenetv2_cifar10/fp32/baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:18,092 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Successfully loaded JIT traced model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/fp32/baseline/jit_trace.pt\n",
      "2025-06-13 16:07:18,113 - nnopt.model.export - INFO - Starting ONNX export to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_baseline_fp32.onnx with opset_version=13...\n",
      "/home/pbeuran/repos/nnopt/.venv/lib/python3.12/site-packages/torch/onnx/utils.py:814: UserWarning: no signature found for builtin <built-in method __call__ of PyCapsule object at 0x71cfd43e9170>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting baseline FP32 model to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_baseline_fp32.onnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:18,412 - nnopt.model.export - INFO - Model successfully exported to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_baseline_fp32.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline FP32 model exported successfully.\n",
      "ONNX Runtime (FP32 Baseline) output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load the baseline FP32 model\n",
    "print(f\"Loading baseline FP32 model from version: {baseline_fp32_version}\")\n",
    "baseline_fp32_model, _ = load_mobilenetv2_cifar10_model(\n",
    "    version=baseline_fp32_version,\n",
    "    mode=\"jit_trace\"\n",
    ")\n",
    "baseline_fp32_model.eval()\n",
    "baseline_fp32_model.to('cpu') # Move model to CPU for export\n",
    "\n",
    "# Define ONNX path\n",
    "onnx_path_baseline_fp32 = os.path.join(ONNX_EXPORT_DIR, \"mobilenetv2_cifar10_baseline_fp32.onnx\")\n",
    "\n",
    "# Export to ONNX\n",
    "print(f\"Exporting baseline FP32 model to {onnx_path_baseline_fp32}...\")\n",
    "success_fp32 = export_model_to_onnx(\n",
    "    model=baseline_fp32_model,\n",
    "    dummy_input=dummy_input,\n",
    "    onnx_path=onnx_path_baseline_fp32,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=13\n",
    ")\n",
    "\n",
    "if success_fp32:\n",
    "    print(\"Baseline FP32 model exported successfully.\")\n",
    "    # Run inference with ONNX Runtime\n",
    "    try:\n",
    "        ort_session_fp32 = ort.InferenceSession(onnx_path_baseline_fp32, providers=['CPUExecutionProvider'])\n",
    "        input_name_fp32 = ort_session_fp32.get_inputs()[0].name\n",
    "        output_name_fp32 = ort_session_fp32.get_outputs()[0].name\n",
    "        \n",
    "        ort_inputs_fp32 = {input_name_fp32: dummy_input.cpu().numpy()}\n",
    "        ort_outputs_fp32 = ort_session_fp32.run([output_name_fp32], ort_inputs_fp32)\n",
    "        print(f\"ONNX Runtime (FP32 Baseline) output shape: {ort_outputs_fp32[0].shape}\")\n",
    "        # print(f\"ONNX Runtime (FP32 Baseline) output sample: {ort_outputs_fp32[0][0,:5]}\") # Print first 5 logits\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ONNX Runtime for FP32 baseline model: {e}\")\n",
    "else:\n",
    "    print(\"Baseline FP32 model export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae62bd9",
   "metadata": {},
   "source": [
    "### Evaluate Baseline FP32 ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c372a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:20,533 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading existing training and validation datasets...\n",
      "2025-06-13 16:07:22,122 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading existing test dataset...\n",
      "2025-06-13 16:07:22,277 - nnopt.model.eval - INFO - Starting ONNX model evaluation for: /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_baseline_fp32.onnx\n",
      "2025-06-13 16:07:22,277 - nnopt.model.eval - INFO - Evaluation on PyTorch device: cpu, batch size: 32\n",
      "2025-06-13 16:07:22,278 - nnopt.model.eval - INFO - Using ONNX Runtime providers: ['CPUExecutionProvider']\n",
      "2025-06-13 16:07:22,300 - nnopt.model.eval - INFO - ONNX Model Input Name: input, Output Name: output\n",
      "2025-06-13 16:07:22,300 - nnopt.model.eval - INFO - Starting warmup for 2 batches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Baseline FP32 ONNX Model on CPU ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ONNX Warmup]: 100%|██████████| 2/2 [00:00<00:00,  5.24it/s]\n",
      "2025-06-13 16:07:22,750 - nnopt.model.eval - INFO - Warmup complete.\n",
      "2025-06-13 16:07:22,751 - nnopt.model.eval - INFO - Starting ONNX model evaluation pass...\n",
      "[ONNX Evaluation]: 100%|██████████| 157/157 [00:16<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Evaluation Complete: Avg Loss: 0.2173, Accuracy: 0.9248\n",
      "Throughput: 421.06 samples/sec | Avg Batch Time: 75.64 ms | Avg Sample Time: 2.37 ms\n",
      "System Stats (PyTorch side): CPU Usage: 64.40% | RAM Usage: 8.9/30.9GB (37.4%)\n",
      "CPU ONNX Metrics: {'accuracy': 0.9248, 'avg_loss': 0.21728210688829422, 'samples_per_second': 421.0579300657744, 'avg_time_per_batch': 0.0756359908789436, 'avg_time_per_sample': 0.002374970113598829}\n",
      "\n",
      "Skipping GPU ONNX evaluation as CUDA is not available or ONNX Runtime GPU provider is not set up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions and data\n",
    "from nnopt.model.eval import eval_onnx_model\n",
    "from nnopt.recipes.mobilenetv2_cifar10 import get_cifar10_datasets\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "# Assuming the data is already downloaded and preprocessed as in other notebooks\n",
    "# Adjust data_dir if your CIFAR-10 data is located elsewhere\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'image', 'cifar10') \n",
    "_, test_dataset, _ = get_cifar10_datasets() # We only need test_dataset\n",
    "\n",
    "if success_fp32: # Only proceed if the ONNX model was exported successfully\n",
    "    print(\"\\n--- Evaluating Baseline FP32 ONNX Model on CPU ---\")\n",
    "    onnx_metrics_cpu = eval_onnx_model(\n",
    "        onnx_model_path=onnx_path_baseline_fp32,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=32, # Adjust as needed\n",
    "        device=\"cpu\",\n",
    "        num_warmup_batches=2 # Smaller warmup for quicker testing\n",
    "    )\n",
    "    print(f\"CPU ONNX Metrics: {onnx_metrics_cpu}\")\n",
    "\n",
    "    if torch.cuda.is_available() and ort.get_device() == 'GPU':\n",
    "        print(\"\\n--- Evaluating Baseline FP32 ONNX Model on GPU ---\")\n",
    "        onnx_metrics_gpu = eval_onnx_model(\n",
    "            onnx_model_path=onnx_path_baseline_fp32,\n",
    "            test_dataset=test_dataset,\n",
    "            batch_size=32, # Adjust as needed\n",
    "            device=\"cuda\",\n",
    "            num_warmup_batches=2\n",
    "        )\n",
    "        print(f\"GPU ONNX Metrics: {onnx_metrics_gpu}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping GPU ONNX evaluation as CUDA is not available or ONNX Runtime GPU provider is not set up.\")\n",
    "else:\n",
    "    print(\"\\nSkipping ONNX model evaluation as the export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4166a",
   "metadata": {},
   "source": [
    "## 2. Baseline QAT INT8 Model Export & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c018d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:38,823 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading MobileNetV2 model for CIFAR-10 from version: mobilenetv2_cifar10/int8/qat_baseline at /home/pbeuran/repos/nnopt/models\n",
      "2025-06-13 16:07:38,824 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loaded metadata: {'metrics_values': {'accuracy': 0.863, 'avg_loss': 0.41772424149513243, 'samples_per_second': 255.84980960873506, 'avg_time_per_batch': 0.1244758939108254, 'avg_time_per_sample': 0.003908543068799918, 'params_stats': {'int_weight_params': 2202560, 'float_weight_params': 0, 'float_bias_params': 17066, 'bn_param_params': 0, 'other_float_params': 0, 'total_params': 2219626, 'approx_memory_mb_for_params': 2.1656265258789062}}}\n",
      "2025-06-13 16:07:38,824 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading JIT traced model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/int8/qat_baseline/jit_trace.pt\n",
      "2025-06-13 16:07:38,978 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Successfully loaded JIT traced model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/int8/qat_baseline/jit_trace.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Baseline QAT INT8 model from version: mobilenetv2_cifar10/int8/qat_baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:38,983 - nnopt.model.export - INFO - Starting ONNX export to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_qat_int8.onnx with opset_version=13...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Baseline QAT INT8 model to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_qat_int8.onnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbeuran/repos/nnopt/.venv/lib/python3.12/site-packages/torch/onnx/utils.py:814: UserWarning: no signature found for builtin <built-in method __call__ of PyCapsule object at 0x71cfd43e9170>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n",
      "/home/pbeuran/repos/nnopt/.venv/lib/python3.12/site-packages/torch/jit/_trace.py:685: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\n",
      "  warnings.warn(\n",
      "2025-06-13 16:07:39,414 - nnopt.model.export - INFO - Model successfully exported to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_qat_int8.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline QAT INT8 model exported successfully.\n",
      "ONNX Runtime (QAT INT8) output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load the Baseline QAT INT8 model\n",
    "print(f\"Loading Baseline QAT INT8 model from version: {qat_int8_version}\")\n",
    "# The QAT model is saved after torch.quantization.convert, so it's already an INT8 model.\n",
    "# We use quantized=True in get_mobilenetv2_cifar10_model to load the correct model architecture\n",
    "# (e.g., torchvision.models.quantization.mobilenet_v2)\n",
    "qat_int8_model, _ = load_mobilenetv2_cifar10_model(\n",
    "    version=qat_int8_version,\n",
    "    device='cpu', # QAT models are typically exported on CPU\n",
    "    mode=\"jit_trace\"\n",
    ")\n",
    "qat_int8_model.eval()\n",
    "qat_int8_model.to('cpu') # Quantized models run on CPU. ONNX export also expects CPU model.\n",
    "\n",
    "# Define ONNX path\n",
    "onnx_path_qat_int8 = os.path.join(ONNX_EXPORT_DIR, \"mobilenetv2_cifar10_qat_int8.onnx\")\n",
    "\n",
    "# Export to ONNX\n",
    "# Opset version 13+ is generally recommended for better support of quantized operators.\n",
    "print(f\"Exporting Baseline QAT INT8 model to {onnx_path_qat_int8}...\")\n",
    "success_qat_int8 = export_model_to_onnx(\n",
    "    model=qat_int8_model,\n",
    "    dummy_input=dummy_input, # Dummy input should be FP32 for QAT model export\n",
    "    onnx_path=onnx_path_qat_int8,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=13 # Use opset 13 or higher for QAT models\n",
    ")\n",
    "\n",
    "if success_qat_int8:\n",
    "    print(\"Baseline QAT INT8 model exported successfully.\")\n",
    "    # Run inference with ONNX Runtime\n",
    "    try:\n",
    "        ort_session_qat_int8 = ort.InferenceSession(onnx_path_qat_int8, providers=['CPUExecutionProvider'])\n",
    "        input_name_qat_int8 = ort_session_qat_int8.get_inputs()[0].name\n",
    "        output_name_qat_int8 = ort_session_qat_int8.get_outputs()[0].name\n",
    "        \n",
    "        # Input to ONNX Runtime for QAT model is also FP32\n",
    "        ort_inputs_qat_int8 = {input_name_qat_int8: dummy_input.cpu().numpy()}\n",
    "        ort_outputs_qat_int8 = ort_session_qat_int8.run([output_name_qat_int8], ort_inputs_qat_int8)\n",
    "        print(f\"ONNX Runtime (QAT INT8) output shape: {ort_outputs_qat_int8[0].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ONNX Runtime for QAT INT8 model: {e}\")\n",
    "else:\n",
    "    print(\"Baseline QAT INT8 model export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2c29c",
   "metadata": {},
   "source": [
    "### Evaluate Baseline QAT INT8 ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935838e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:39,462 - nnopt.model.eval - INFO - Starting ONNX model evaluation for: /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_qat_int8.onnx\n",
      "2025-06-13 16:07:39,462 - nnopt.model.eval - INFO - Evaluation on PyTorch device: cpu, batch size: 32\n",
      "2025-06-13 16:07:39,463 - nnopt.model.eval - INFO - Using ONNX Runtime providers: ['CPUExecutionProvider']\n",
      "2025-06-13 16:07:39,493 - nnopt.model.eval - INFO - ONNX Model Input Name: input, Output Name: output\n",
      "2025-06-13 16:07:39,494 - nnopt.model.eval - INFO - Starting warmup for 2 batches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Baseline QAT INT8 ONNX Model on CPU ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ONNX Warmup]: 100%|██████████| 2/2 [00:00<00:00,  7.45it/s]\n",
      "2025-06-13 16:07:39,835 - nnopt.model.eval - INFO - Warmup complete.\n",
      "2025-06-13 16:07:39,835 - nnopt.model.eval - INFO - Starting ONNX model evaluation pass...\n",
      "[ONNX Evaluation]: 100%|██████████| 157/157 [00:09<00:00, 17.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Evaluation Complete: Avg Loss: 0.4202, Accuracy: 0.8618\n",
      "Throughput: 844.57 samples/sec | Avg Batch Time: 37.71 ms | Avg Sample Time: 1.18 ms\n",
      "System Stats (PyTorch side): CPU Usage: 89.40% | RAM Usage: 8.6/30.9GB (36.6%)\n",
      "CPU ONNX Metrics (QAT INT8): {'accuracy': 0.8618, 'avg_loss': 0.4202093836784363, 'samples_per_second': 844.5694715047395, 'avg_time_per_batch': 0.03770812802553812, 'avg_time_per_sample': 0.0011840352200018969}\n",
      "\n",
      "Skipping GPU ONNX evaluation for QAT INT8 model as CUDA is not available or ONNX Runtime GPU provider is not set up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure test_dataset is loaded\n",
    "\n",
    "if success_qat_int8: # Only proceed if the ONNX model was exported successfully\n",
    "    print(\"\\n--- Evaluating Baseline QAT INT8 ONNX Model on CPU ---\")\n",
    "    onnx_metrics_qat_int8_cpu = eval_onnx_model(\n",
    "        onnx_model_path=onnx_path_qat_int8,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=32, \n",
    "        device=\"cpu\", # QAT models are typically evaluated on CPU\n",
    "        num_warmup_batches=2\n",
    "    )\n",
    "    print(f\"CPU ONNX Metrics (QAT INT8): {onnx_metrics_qat_int8_cpu}\")\n",
    "\n",
    "    # Optional: Test QAT INT8 on GPU if supported and desired\n",
    "    # Note: GPU support for INT8 ONNX models can be more complex and might require specific ONNX opset versions\n",
    "    # or specific GPU capabilities and ONNX Runtime build options.\n",
    "    if torch.cuda.is_available() and ort.get_device() == 'GPU':\n",
    "        print(\"\\n--- Evaluating Baseline QAT INT8 ONNX Model on GPU (Experimental) ---\")\n",
    "        try:\n",
    "            onnx_metrics_qat_int8_gpu = eval_onnx_model(\n",
    "                onnx_model_path=onnx_path_qat_int8,\n",
    "                test_dataset=test_dataset,\n",
    "                batch_size=32, \n",
    "                device=\"cuda\",\n",
    "                num_warmup_batches=2\n",
    "            )\n",
    "            print(f\"GPU ONNX Metrics (QAT INT8): {onnx_metrics_qat_int8_gpu}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not run QAT INT8 ONNX model on GPU: {e}\")\n",
    "            print(\"This might be due to operator support or other configuration issues.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping GPU ONNX evaluation for QAT INT8 model as CUDA is not available or ONNX Runtime GPU provider is not set up.\")\n",
    "else:\n",
    "    print(\"\\nSkipping ONNX model evaluation for QAT INT8 model as the export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95bf6f",
   "metadata": {},
   "source": [
    "## 3. L1-structured Pruning FP32 Model Export & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0bb0daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:48,970 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading MobileNetV2 model for CIFAR-10 from version: mobilenetv2_cifar10/fp32/l1_struct_prune_0.3 at /home/pbeuran/repos/nnopt/models\n",
      "2025-06-13 16:07:48,972 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loaded metadata: {'metrics_values': {'val_metrics': {'accuracy': 0.7892, 'avg_loss': 0.6139473709106446, 'samples_per_second': 9049.953083963761, 'avg_time_per_batch': 0.006993532303792413, 'avg_time_per_sample': 0.00011049781039992013, 'params_stats': {'int_weight_params': 0, 'float_weight_params': 584051, 'float_bias_params': 10, 'bn_param_params': 17100, 'other_float_params': 0, 'total_params': 601161, 'approx_memory_mb_for_params': 2.2932472229003906}}, 'test_metrics': {'accuracy': 0.7872, 'avg_loss': 0.6139407681465149, 'samples_per_second': 10497.233072640314, 'avg_time_per_batch': 0.006067719662425565, 'avg_time_per_sample': 9.526319870008137e-05, 'params_stats': {'int_weight_params': 0, 'float_weight_params': 584051, 'float_bias_params': 10, 'bn_param_params': 17100, 'other_float_params': 0, 'total_params': 601161, 'approx_memory_mb_for_params': 2.2932472229003906}}, 'relative_sparsity': 0.26877356727509766}}\n",
      "2025-06-13 16:07:48,972 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading JIT traced model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/fp32/l1_struct_prune_0.3/jit_trace.pt\n",
      "2025-06-13 16:07:49,086 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Successfully loaded JIT traced model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/fp32/l1_struct_prune_0.3/jit_trace.pt\n",
      "2025-06-13 16:07:49,106 - nnopt.model.export - INFO - Starting ONNX export to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_pruned_fp32.onnx with opset_version=13...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading l1 structured pruned FP32 model from version: mobilenetv2_cifar10/fp32/l1_struct_prune_0.3\n",
      "Exporting pruned FP32 model to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_pruned_fp32.onnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:49,311 - nnopt.model.export - INFO - Model successfully exported to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_pruned_fp32.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned FP32 model exported successfully.\n",
      "ONNX Runtime (FP32 Pruned) output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load the l1 structured pruned FP32 model \n",
    "print(f\"Loading l1 structured pruned FP32 model from version: {struct_pruned_fp32_version}\")\n",
    "struct_pruned_fp32_model, _ = load_mobilenetv2_cifar10_model(\n",
    "    version=struct_pruned_fp32_version,\n",
    "    mode=\"jit_trace\"\n",
    ")\n",
    "struct_pruned_fp32_model.eval()\n",
    "struct_pruned_fp32_model.to('cpu') # Move model to CPU for export\n",
    "\n",
    "# Define ONNX path\n",
    "onnx_path_pruned_fp32 = os.path.join(ONNX_EXPORT_DIR, \"mobilenetv2_cifar10_pruned_fp32.onnx\")\n",
    "\n",
    "# Export to ONNX\n",
    "print(f\"Exporting pruned FP32 model to {onnx_path_pruned_fp32}...\")\n",
    "success_fp32 = export_model_to_onnx(\n",
    "    model=struct_pruned_fp32_model,\n",
    "    dummy_input=dummy_input,\n",
    "    onnx_path=onnx_path_pruned_fp32,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=13\n",
    ")\n",
    "\n",
    "if success_fp32:\n",
    "    print(\"Pruned FP32 model exported successfully.\")\n",
    "    # Run inference with ONNX Runtime\n",
    "    try:\n",
    "        ort_session_fp32 = ort.InferenceSession(onnx_path_pruned_fp32, providers=['CPUExecutionProvider'])\n",
    "        input_name_fp32 = ort_session_fp32.get_inputs()[0].name\n",
    "        output_name_fp32 = ort_session_fp32.get_outputs()[0].name\n",
    "        \n",
    "        ort_inputs_fp32 = {input_name_fp32: dummy_input.cpu().numpy()}\n",
    "        ort_outputs_fp32 = ort_session_fp32.run([output_name_fp32], ort_inputs_fp32)\n",
    "        print(f\"ONNX Runtime (FP32 Pruned) output shape: {ort_outputs_fp32[0].shape}\")\n",
    "        # print(f\"ONNX Runtime (FP32 Pruned) output sample: {ort_outputs_fp32[0][0,:5]}\") # Print first 5 logits\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ONNX Runtime for FP32 pruned model: {e}\")\n",
    "else:\n",
    "    print(\"Pruned FP32 model export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72d9a1",
   "metadata": {},
   "source": [
    "### Evaluate L1 structured Pruned FP32 ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6069741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:07:49,348 - nnopt.model.eval - INFO - Starting ONNX model evaluation for: /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_pruned_fp32.onnx\n",
      "2025-06-13 16:07:49,348 - nnopt.model.eval - INFO - Evaluation on PyTorch device: cpu, batch size: 32\n",
      "2025-06-13 16:07:49,349 - nnopt.model.eval - INFO - Using ONNX Runtime providers: ['CPUExecutionProvider']\n",
      "2025-06-13 16:07:49,363 - nnopt.model.eval - INFO - ONNX Model Input Name: input, Output Name: output\n",
      "2025-06-13 16:07:49,363 - nnopt.model.eval - INFO - Starting warmup for 2 batches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Pruned FP32 ONNX Model on CPU ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ONNX Warmup]: 100%|██████████| 2/2 [00:00<00:00,  3.18it/s]\n",
      "2025-06-13 16:07:50,080 - nnopt.model.eval - INFO - Warmup complete.\n",
      "2025-06-13 16:07:50,081 - nnopt.model.eval - INFO - Starting ONNX model evaluation pass...\n",
      "[ONNX Evaluation]: 100%|██████████| 157/157 [00:33<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Evaluation Complete: Avg Loss: 0.6140, Accuracy: 0.7890\n",
      "Throughput: 167.63 samples/sec | Avg Batch Time: 189.98 ms | Avg Sample Time: 5.97 ms\n",
      "System Stats (PyTorch side): CPU Usage: 95.30% | RAM Usage: 8.7/30.9GB (36.7%)\n",
      "CPU ONNX Metrics: {'accuracy': 0.789, 'avg_loss': 0.6139658059120178, 'samples_per_second': 167.63468192712253, 'avg_time_per_batch': 0.18997938488532462, 'avg_time_per_sample': 0.005965352685399193}\n",
      "\n",
      "Skipping GPU ONNX evaluation as CUDA is not available or ONNX Runtime GPU provider is not set up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if success_fp32: # Only proceed if the ONNX model was exported successfully\n",
    "    print(\"\\n--- Evaluating Pruned FP32 ONNX Model on CPU ---\")\n",
    "    onnx_metrics_cpu = eval_onnx_model(\n",
    "        onnx_model_path=onnx_path_pruned_fp32,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=32, # Adjust as needed\n",
    "        device=\"cpu\",\n",
    "        num_warmup_batches=2 # Smaller warmup for quicker testing\n",
    "    )\n",
    "    print(f\"CPU ONNX Metrics: {onnx_metrics_cpu}\")\n",
    "\n",
    "    if torch.cuda.is_available() and ort.get_device() == 'GPU':\n",
    "        print(\"\\n--- Evaluating Pruned FP32 ONNX Model on GPU ---\")\n",
    "        onnx_metrics_gpu = eval_onnx_model(\n",
    "            onnx_model_path=onnx_path_baseline_fp32,\n",
    "            test_dataset=test_dataset,\n",
    "            batch_size=32, # Adjust as needed\n",
    "            device=\"cuda\",\n",
    "            num_warmup_batches=2\n",
    "        )\n",
    "        print(f\"GPU ONNX Metrics: {onnx_metrics_gpu}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping GPU ONNX evaluation as CUDA is not available or ONNX Runtime GPU provider is not set up.\")\n",
    "else:\n",
    "    print(\"\\nSkipping ONNX model evaluation as the export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3bced",
   "metadata": {},
   "source": [
    "# 4. L1-unstructured Pruning FP32 Model Export & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "350c212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:08:23,947 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading MobileNetV2 model for CIFAR-10 from version: mobilenetv2_cifar10/fp32/l1_unstruct_prune_0.7 at /home/pbeuran/repos/nnopt/models\n",
      "2025-06-13 16:08:23,949 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loaded metadata: {'unstructured_sparse_config': {'pruning_amount': 0.7}, 'metrics_values': {'val_metrics': {'accuracy': 0.871, 'avg_loss': 0.3521343002319336, 'samples_per_second': 8535.882614956985, 'avg_time_per_batch': 0.007414715278488546, 'avg_time_per_sample': 0.00011715250140011903, 'params_stats': {'int_weight_params': 0, 'float_weight_params': 2202560, 'float_bias_params': 10, 'bn_param_params': 34112, 'other_float_params': 0, 'total_params': 2236682, 'approx_memory_mb_for_params': 8.532264709472656}}, 'test_metrics': {'accuracy': 0.8729, 'avg_loss': 0.3661128273010254, 'samples_per_second': 8581.675884406815, 'avg_time_per_batch': 0.007422124579612489, 'avg_time_per_sample': 0.00011652735589991607, 'params_stats': {'int_weight_params': 0, 'float_weight_params': 2202560, 'float_bias_params': 10, 'bn_param_params': 34112, 'other_float_params': 0, 'total_params': 2236682, 'approx_memory_mb_for_params': 8.532264709472656}}}}\n",
      "2025-06-13 16:08:23,949 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading JIT traced model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/fp32/l1_unstruct_prune_0.7/jit_trace.pt\n",
      "2025-06-13 16:08:24,068 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Successfully loaded JIT traced model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/fp32/l1_unstruct_prune_0.7/jit_trace.pt\n",
      "2025-06-13 16:08:24,090 - nnopt.model.export - INFO - Starting ONNX export to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_unstructured_pruned_fp32.onnx with opset_version=13...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unstructured L1-pruned FP32 model from version: mobilenetv2_cifar10/fp32/l1_unstruct_prune_0.7\n",
      "Exporting unstructured pruned FP32 model to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_unstructured_pruned_fp32.onnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:08:24,344 - nnopt.model.export - INFO - Model successfully exported to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_unstructured_pruned_fp32.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unstructured FP32 model exported successfully.\n",
      "ONNX Runtime (Unstructured FP32) output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(f\"Loading unstructured L1-pruned FP32 model from version: {unstruct_pruned_fp32_version}\")\n",
    "unstruct_pruned_fp32_model, _ = load_mobilenetv2_cifar10_model(\n",
    "    version=unstruct_pruned_fp32_version,\n",
    "    mode=\"jit_trace\"\n",
    ")\n",
    "unstruct_pruned_fp32_model.eval()\n",
    "unstruct_pruned_fp32_model.to('cpu')\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path_unstruct_fp32 = os.path.join(\n",
    "    ONNX_EXPORT_DIR,\n",
    "    \"mobilenetv2_cifar10_unstructured_pruned_fp32.onnx\"\n",
    ")\n",
    "print(f\"Exporting unstructured pruned FP32 model to {onnx_path_unstruct_fp32}...\")\n",
    "success_unstruct = export_model_to_onnx(\n",
    "    model=unstruct_pruned_fp32_model,\n",
    "    dummy_input=dummy_input,\n",
    "    onnx_path=onnx_path_unstruct_fp32,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=13\n",
    ")\n",
    "\n",
    "if success_unstruct:\n",
    "    print(\"Unstructured FP32 model exported successfully.\")\n",
    "    # Create ONNX Runtime session on CPU\n",
    "    ort_session_unstruct = ort.InferenceSession(\n",
    "        onnx_path_unstruct_fp32,\n",
    "        providers=['CPUExecutionProvider']\n",
    "    )\n",
    "    input_name = ort_session_unstruct.get_inputs()[0].name\n",
    "    output_name = ort_session_unstruct.get_outputs()[0].name\n",
    "\n",
    "    # Run a forward pass\n",
    "    ort_inputs = {input_name: dummy_input.cpu().numpy()}\n",
    "    ort_outputs = ort_session_unstruct.run([output_name], ort_inputs)\n",
    "    print(f\"ONNX Runtime (Unstructured FP32) output shape: {ort_outputs[0].shape}\")\n",
    "else:\n",
    "    print(\"Unstructured FP32 model export failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31607e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:08:24,380 - nnopt.model.eval - INFO - Starting ONNX model evaluation for: /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_unstructured_pruned_fp32.onnx\n",
      "2025-06-13 16:08:24,383 - nnopt.model.eval - INFO - Evaluation on PyTorch device: cpu, batch size: 32\n",
      "2025-06-13 16:08:24,383 - nnopt.model.eval - INFO - Using ONNX Runtime providers: ['CPUExecutionProvider']\n",
      "2025-06-13 16:08:24,405 - nnopt.model.eval - INFO - ONNX Model Input Name: input, Output Name: output\n",
      "2025-06-13 16:08:24,406 - nnopt.model.eval - INFO - Starting warmup for 2 batches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Unstructured FP32 ONNX Model on CPU ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ONNX Warmup]: 100%|██████████| 2/2 [00:00<00:00,  6.29it/s]\n",
      "2025-06-13 16:08:24,806 - nnopt.model.eval - INFO - Warmup complete.\n",
      "2025-06-13 16:08:24,806 - nnopt.model.eval - INFO - Starting ONNX model evaluation pass...\n",
      "[ONNX Evaluation]: 100%|██████████| 157/157 [00:15<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Evaluation Complete: Avg Loss: 0.3516, Accuracy: 0.8704\n",
      "Throughput: 440.68 samples/sec | Avg Batch Time: 72.27 ms | Avg Sample Time: 2.27 ms\n",
      "System Stats (PyTorch side): CPU Usage: 94.00% | RAM Usage: 8.8/30.9GB (37.3%)\n",
      "CPU ONNX (Unstructured) Metrics: {'accuracy': 0.8704, 'avg_loss': 0.3515835962295532, 'samples_per_second': 440.67883370405343, 'avg_time_per_batch': 0.07226835355416537, 'avg_time_per_sample': 0.002269226301600793}\n",
      "\n",
      "Skipping GPU ONNX (Unstructured) evaluation as CUDA is not available or ONNX Runtime GPU provider is not set up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluating Unstructured L1‐Pruned FP32 ONNX Model ---\n",
    "\n",
    "if success_unstruct:  # Only proceed if the ONNX export succeeded\n",
    "    print(\"\\n--- Evaluating Unstructured FP32 ONNX Model on CPU ---\")\n",
    "    onnx_metrics_cpu_unstruct = eval_onnx_model(\n",
    "        onnx_model_path=onnx_path_unstruct_fp32,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=32,          # Adjust as needed\n",
    "        device=\"cpu\",\n",
    "        num_warmup_batches=2    # Smaller warmup for quicker testing\n",
    "    )\n",
    "    print(f\"CPU ONNX (Unstructured) Metrics: {onnx_metrics_cpu_unstruct}\")\n",
    "\n",
    "    # GPU path (if available)\n",
    "    if torch.cuda.is_available() and ort.get_device() == 'GPU':\n",
    "        print(\"\\n--- Evaluating Unstructured FP32 ONNX Model on GPU ---\")\n",
    "        onnx_metrics_gpu_unstruct = eval_onnx_model(\n",
    "            onnx_model_path=onnx_path_unstruct_fp32,\n",
    "            test_dataset=test_dataset,\n",
    "            batch_size=32,\n",
    "            device=\"cuda\",\n",
    "            num_warmup_batches=2\n",
    "        )\n",
    "        print(f\"GPU ONNX (Unstructured) Metrics: {onnx_metrics_gpu_unstruct}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping GPU ONNX (Unstructured) evaluation as CUDA is not available or ONNX Runtime GPU provider is not set up.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Unstructured ONNX model evaluation as the export failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f47cbc",
   "metadata": {},
   "source": [
    "# 5. L1-unstructured Pruning QAT INT8 Model Export & Inference (with OpenVINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "954fb6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO Sparse Inference output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# --- OpenVINO inference with sparse acceleration for unstructured pruned model ---\n",
    "\n",
    "from openvino.runtime import Core\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the already-exported ONNX file:\n",
    "onnx_unstruct_fp32 = os.path.join(\n",
    "    ONNX_EXPORT_DIR,\n",
    "    \"mobilenetv2_cifar10_unstructured_pruned_fp32.onnx\"\n",
    ")\n",
    "\n",
    "# Initialize OpenVINO runtime\n",
    "ie = Core()\n",
    "\n",
    "# Read the ONNX model\n",
    "ov_model = ie.read_model(onnx_unstruct_fp32)\n",
    "\n",
    "# Compile with sparse-weight decompression enabled when at least 50% weights are zero\n",
    "compiled_model = ie.compile_model(\n",
    "    model=ov_model,\n",
    "    device_name=\"CPU\",\n",
    "    config={\"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\": \"0.5\"}\n",
    ")\n",
    "\n",
    "# Prepare input as NumPy\n",
    "input_array = dummy_input.cpu().numpy()\n",
    "\n",
    "# Inference\n",
    "results = compiled_model([input_array])\n",
    "\n",
    "print(f\"OpenVINO Sparse Inference output shape: {results[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2fcd41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Unstructured FP32 Model with OpenVINO Sparse Acceleration ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[OpenVINO Eval]: 100%|██████████| 157/157 [00:04<00:00, 37.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO Sparse Metrics: {'accuracy': 0.8726, 'avg_loss': 0.3507337382555008, 'samples_per_second': 1340.0696910670774, 'avg_time_per_batch': 0.023765281738894032, 'avg_time_per_sample': 0.0007462298466012725}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluating Unstructured L1‐Pruned FP32 Model with OpenVINO Sparse Acceleration (fixed) ---\n",
    "from tqdm import tqdm\n",
    "from nnopt.model.eval import eval_model_openvino\n",
    "\n",
    "if success_unstruct:\n",
    "    print(\"\\n--- Evaluating Unstructured FP32 Model with OpenVINO Sparse Acceleration ---\")\n",
    "    ov_metrics = eval_model_openvino(\n",
    "        onnx_model_path=onnx_path_unstruct_fp32,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=32,             # match your earlier config\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        sparse_rate=0.7,           # set to your actual sparsity threshold\n",
    "        num_warmup_batches=2,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"OpenVINO Sparse Metrics: {ov_metrics}\")\n",
    "else:\n",
    "    print(\"Skipping OpenVINO evaluation as the ONNX export failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853244b2",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d45291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from nnopt.model.eval import eval_model # For PyTorch model evaluation\n",
    "from nnopt.recipes.mobilenetv2_cifar10 import init_mobilenetv2_cifar10_model, get_cifar10_datasets # To load models and dataset\n",
    "\n",
    "# Ensure test_dataset is loaded (it should be from earlier cells, e.g., cell d52e6d98)\n",
    "# If not, uncomment and run:\n",
    "# DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'image', 'cifar10')\n",
    "# _, _, test_dataset = get_cifar10_datasets(data_dir=DATA_DIR) # Or however you load it\n",
    "\n",
    "# --- 1. PyTorch FP32 Baseline Model Evaluation (CPU) ---\n",
    "print(\"Evaluating PyTorch FP32 Baseline Model on CPU for analysis...\")\n",
    "pytorch_fp32_model, _ = init_mobilenetv2_cifar10_model(\n",
    "    version=baseline_fp32_version, # Defined in cell f411f025\n",
    "    device='cpu'\n",
    ")\n",
    "pytorch_fp32_model.eval()\n",
    "pytorch_fp32_metrics_cpu = eval_model(\n",
    "    model=pytorch_fp32_model,\n",
    "    test_dataset=test_dataset,\n",
    "    device=\"cpu\",\n",
    "    use_amp=False, # No AMP for CPU FP32\n",
    "    dtype=torch.float32,\n",
    "    batch_size=32, # Consistent batch size\n",
    "    num_warmup_batches=2\n",
    ")\n",
    "print(f\"PyTorch FP32 CPU Metrics: {pytorch_fp32_metrics_cpu}\")\n",
    "\n",
    "# --- 2. PyTorch QAT INT8 Baseline Model Evaluation (CPU) ---\n",
    "print(\"\\nEvaluating PyTorch QAT INT8 Baseline Model on CPU for analysis...\")\n",
    "# qat_int8_version is defined in cell f411f025\n",
    "pytorch_qat_int8_model, _ = init_mobilenetv2_cifar10_model(\n",
    "    version=qat_int8_version,\n",
    "    device='cpu'\n",
    ")\n",
    "pytorch_qat_int8_model.eval()\n",
    "pytorch_qat_int8_metrics_cpu = eval_model(\n",
    "    model=pytorch_qat_int8_model,\n",
    "    test_dataset=test_dataset,\n",
    "    device=\"cpu\",\n",
    "    use_amp=False, # QAT models are typically run with FP32 interface, actual ops are INT8\n",
    "    dtype=torch.float32,\n",
    "    batch_size=32,\n",
    "    num_warmup_batches=2\n",
    ")\n",
    "print(f\"PyTorch QAT INT8 CPU Metrics: {pytorch_qat_int8_metrics_cpu}\")\n",
    "\n",
    "# --- 3. Retrieve ONNX Model Metrics (Assumed to be available from previous cells) ---\n",
    "# Ensure 'onnx_metrics_cpu' and 'onnx_metrics_qat_int8_cpu' are populated from earlier cells\n",
    "# These should contain keys like 'accuracy', 'avg_time_per_sample'\n",
    "print(f\"\\nUsing pre-calculated ONNX FP32 CPU Metrics: {onnx_metrics_cpu}\")\n",
    "print(f\"Using pre-calculated ONNX QAT INT8 CPU Metrics: {onnx_metrics_qat_int8_cpu}\")\n",
    "\n",
    "\n",
    "# --- 4. Model Sizes ---\n",
    "# PyTorch model sizes (from parameters, does not include quantization overhead directly but reflects param precision)\n",
    "# For a more direct comparison with ONNX file size, you could save the PyTorch models and get file size.\n",
    "# Here, we use the parameter-based approximation for PyTorch models.\n",
    "pytorch_fp32_size_mb = pytorch_fp32_metrics_cpu['params_stats']['total_params'] * pytorch_fp32_metrics_cpu['params_stats']['approx_memory_mb_for_params'] / pytorch_fp32_metrics_cpu['params_stats']['total_params'] if pytorch_fp32_metrics_cpu['params_stats']['total_params'] > 0 else 0\n",
    "# For QAT INT8, the parameters are still stored in FP32 for training, but effective size is smaller.\n",
    "# The 'params_stats' from eval_model for a quantized model might still reflect FP32 storage if not careful.\n",
    "# A better measure for PyTorch quantized model size is to save it and check file size, or estimate based on INT8.\n",
    "# For simplicity, we'll use the reported size from eval_model, but acknowledge it might be an overestimate for QAT.\n",
    "# A more accurate way for PyTorch INT8 model size:\n",
    "torch.save(pytorch_qat_int8_model.state_dict(), \"temp_qat_int8_model.pth\")\n",
    "pytorch_qat_int8_size_mb = os.path.getsize(\"temp_qat_int8_model.pth\") / (1024 * 1024)\n",
    "os.remove(\"temp_qat_int8_model.pth\")\n",
    "print(f\"PyTorch QAT INT8 Model Size (saved state_dict): {pytorch_qat_int8_size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "# ONNX model file sizes\n",
    "# Ensure 'onnx_path_baseline_fp32' and 'onnx_path_qat_int8' are defined (cell f411f025 and 5b5fb4cd, 2170bb29)\n",
    "onnx_fp32_size_mb = os.path.getsize(onnx_path_baseline_fp32) / (1024 * 1024) if os.path.exists(onnx_path_baseline_fp32) else 0\n",
    "onnx_qat_int8_size_mb = os.path.getsize(onnx_path_qat_int8) / (1024 * 1024) if os.path.exists(onnx_path_qat_int8) else 0\n",
    "print(f\"ONNX FP32 Model Size: {onnx_fp32_size_mb:.2f} MB\")\n",
    "print(f\"ONNX QAT INT8 Model Size: {onnx_qat_int8_size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "# --- 5. Prepare data for plotting ---\n",
    "model_labels = [\n",
    "    \"PyTorch FP32\",\n",
    "    \"PyTorch QAT INT8\",\n",
    "    \"ONNX FP32\",\n",
    "    \"ONNX QAT INT8\"\n",
    "]\n",
    "\n",
    "# Using test accuracies\n",
    "accuracies = [\n",
    "    pytorch_fp32_metrics_cpu['accuracy'],\n",
    "    pytorch_qat_int8_metrics_cpu['accuracy'],\n",
    "    onnx_metrics_cpu['accuracy'] if 'onnx_metrics_cpu' in locals() and onnx_metrics_cpu else 0, # from cell d52e6d98\n",
    "    onnx_metrics_qat_int8_cpu['accuracy'] if 'onnx_metrics_qat_int8_cpu' in locals() and onnx_metrics_qat_int8_cpu else 0 # from cell 8b57290a\n",
    "]\n",
    "\n",
    "# CPU inference time per sample (test set)\n",
    "cpu_time_per_sample = [\n",
    "    pytorch_fp32_metrics_cpu['avg_time_per_sample'],\n",
    "    pytorch_qat_int8_metrics_cpu['avg_time_per_sample'],\n",
    "    onnx_metrics_cpu['avg_time_per_sample'] if 'onnx_metrics_cpu' in locals() and onnx_metrics_cpu else float('inf'),\n",
    "    onnx_metrics_qat_int8_cpu['avg_time_per_sample'] if 'onnx_metrics_qat_int8_cpu' in locals() and onnx_metrics_qat_int8_cpu else float('inf')\n",
    "]\n",
    "\n",
    "# Model sizes in MB\n",
    "model_sizes_mb = [\n",
    "    pytorch_fp32_size_mb,\n",
    "    pytorch_qat_int8_size_mb, # Using saved state_dict size\n",
    "    onnx_fp32_size_mb,\n",
    "    onnx_qat_int8_size_mb\n",
    "]\n",
    "\n",
    "print(\"\\nData for plotting:\")\n",
    "print(f\"Labels: {model_labels}\")\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "print(f\"CPU Time/Sample (s): {cpu_time_per_sample}\")\n",
    "print(f\"Model Sizes (MB): {model_sizes_mb}\")\n",
    "\n",
    "# Check if all ONNX metrics were loaded correctly\n",
    "if not ('onnx_metrics_cpu' in locals() and onnx_metrics_cpu and \\\n",
    "        'onnx_metrics_qat_int8_cpu' in locals() and onnx_metrics_qat_int8_cpu):\n",
    "    print(\"\\nWARNING: ONNX metrics might not be fully loaded. Plots might be incomplete or show zero/infinity values.\")\n",
    "    print(\"Please ensure the cells evaluating ONNX models (d52e6d98, 8b57290a) have been run successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Bar Plot (Test Set on CPU)\n",
    "x = np.arange(len(model_labels))\n",
    "width = 0.5 # Single bar for test accuracy\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects = ax.bar(x, accuracies, width, label='Test Accuracy (CPU)')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Test Accuracy Comparison (CPU)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.set_ylim(min(accuracies) * 0.9 if min(accuracies) > 0 else 0, max(accuracies) * 1.1 if max(accuracies) > 0 else 1) # Adjust y-lim dynamically\n",
    "\n",
    "def autolabel(rects_to_label, ax_to_use):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects_to_label:\n",
    "        height = rect.get_height()\n",
    "        ax_to_use.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "autolabel(rects, ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8702fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Inference Time Comparison Plot (Time per Sample)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Convert times to milliseconds for better readability if they are very small\n",
    "cpu_time_per_sample_ms = [t * 1000 for t in cpu_time_per_sample]\n",
    "rects = ax.bar(x, cpu_time_per_sample_ms, width, label='CPU Time/Sample (ms)')\n",
    "\n",
    "ax.set_ylabel('CPU Time/Sample (milliseconds)')\n",
    "ax.set_title('Model Inference Time Comparison (CPU)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "# ax.set_yscale('log') # Use log scale if times vary greatly\n",
    "\n",
    "def autolabel_time(rects_to_label, ax_to_use):\n",
    "    for rect in rects_to_label:\n",
    "        height = rect.get_height()\n",
    "        ax_to_use.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "autolabel_time(rects, ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Size Comparison Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects = ax.bar(x, model_sizes_mb, width, label='Model Size (MB)')\n",
    "\n",
    "ax.set_ylabel('Model Size (MB)')\n",
    "ax.set_title('Model Size Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "def autolabel_size(rects_to_label, ax_to_use):\n",
    "    for rect in rects_to_label:\n",
    "        height = rect.get_height()\n",
    "        ax_to_use.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "autolabel_size(rects, ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
