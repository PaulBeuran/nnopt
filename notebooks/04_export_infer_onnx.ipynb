{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d23731",
   "metadata": {},
   "source": [
    "# 4. Exporting Models to ONNX and Running Inference\n",
    "\n",
    "This notebook demonstrates how to export the trained/optimized PyTorch models to the ONNX (Open Neural Network Exchange) format. It also shows how to run inference using ONNX Runtime on the exported models.\n",
    "\n",
    "We will export three specific model versions:\n",
    "1.  **Baseline FP32 Model**: The original MobileNetV2 model adapted for CIFAR-10.\n",
    "3.  **Baseline QAT INT8 Model**: The baseline model quantized to INT8 using Quantization-Aware Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b7098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:15:17,157 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Using device: cuda, dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "ONNX Runtime version: 1.22.0\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "from nnopt.model.export import export_model_to_onnx\n",
    "from nnopt.recipes.mobilenetv2_cifar10 import get_mobilenetv2_cifar10_model\n",
    "from nnopt.model.prune import remove_pruning_reparameterization\n",
    "from nnopt.model.const import BASE_MODEL_DIR, DEVICE\n",
    "\n",
    "# Ensure the logger in export is configured (if not already by its import)\n",
    "import logging\n",
    "logger = logging.getLogger(\"nnopt.model.export\")\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "                        handlers=[logging.StreamHandler()])\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1915f",
   "metadata": {},
   "source": [
    "## Configuration and Dummy Input\n",
    "\n",
    "Define the paths for the models and a dummy input tensor required for ONNX export. The dummy input shape should match the expected input of the MobileNetV2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5d8004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX models will be saved in: /home/pbeuran/repos/nnopt/models/onnx_exports\n",
      "Dummy input shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Define model versions to be exported\n",
    "baseline_fp32_version = \"mobilenetv2_cifar10/fp32/baseline\"\n",
    "pruned_fp32_version = \"mobilenetv2_cifar10/fp32/l1_unstruct_prune_0.7\"\n",
    "# The QAT model was saved with a nested version structure in 03_quantization\n",
    "# Original version used for QAT: \"mobilenetv2_cifar10/fp32/baseline\"\n",
    "# Saved QAT version: \"mobilenetv2_cifar10/int8/qat_mobilenetv2_cifar10/fp32/baseline\"\n",
    "qat_int8_version = \"mobilenetv2_cifar10/int8/qat_mobilenetv2_cifar10/fp32/baseline\"\n",
    "\n",
    "\n",
    "# Directory to save ONNX models\n",
    "ONNX_EXPORT_DIR = os.path.join(BASE_MODEL_DIR, \"onnx_exports\")\n",
    "os.makedirs(ONNX_EXPORT_DIR, exist_ok=True)\n",
    "print(f\"ONNX models will be saved in: {ONNX_EXPORT_DIR}\")\n",
    "\n",
    "# Create a dummy input tensor (batch_size, channels, height, width)\n",
    "# MobileNetV2 typically expects 224x224 images.\n",
    "# CIFAR-10 images are 32x32, but the model adapts them or uses a standard input size.\n",
    "# Using 224x224 as per common MobileNetV2 usage and example in pruning notebook.\n",
    "dummy_input_shape = (1, 3, 224, 224)\n",
    "dummy_input = torch.randn(dummy_input_shape, device='cpu') # ONNX export prefers CPU dummy input\n",
    "print(f\"Dummy input shape: {dummy_input.shape}\")\n",
    "\n",
    "# Define dynamic axes for batch size flexibility\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdfd51d",
   "metadata": {},
   "source": [
    "## 1. Baseline FP32 Model Export & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dbab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:15:17,188 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading MobileNetV2 model for CIFAR-10 (architecture and weights) from version: mobilenetv2_cifar10/fp32/baseline at /home/pbeuran/repos/nnopt/models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline FP32 model from version: mobilenetv2_cifar10/fp32/baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:15:17,418 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Successfully loaded model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/fp32/baseline/model.pt\n",
      "2025-06-11 15:15:17,420 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loaded metadata: {'metrics_values': {'val_metrics': {'accuracy': 0.8988, 'avg_loss': 0.2952394811630249, 'samples_per_second': 8822.506935259542, 'avg_time_per_batch': 0.007173827088484393, 'avg_time_per_sample': 0.00011334646799805342, 'params_stats': {'int_weight_params': 0, 'float_weight_params': 2202560, 'float_bias_params': 10, 'bn_param_params': 34112, 'other_float_params': 0, 'total_params': 2236682, 'approx_memory_mb_for_params': 8.532264709472656}}, 'test_metrics': {'accuracy': 0.9004, 'avg_loss': 0.29234390649795533, 'samples_per_second': 8450.564700334979, 'avg_time_per_batch': 0.007537279433337601, 'avg_time_per_sample': 0.00011833528710340034, 'params_stats': {'int_weight_params': 0, 'float_weight_params': 2202560, 'float_bias_params': 10, 'bn_param_params': 34112, 'other_float_params': 0, 'total_params': 2236682, 'approx_memory_mb_for_params': 8.532264709472656}}}}\n",
      "2025-06-11 15:15:17,448 - nnopt.model.export - INFO - Starting ONNX export to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_baseline_fp32.onnx with opset_version=13...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting baseline FP32 model to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_baseline_fp32.onnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:15:17,894 - nnopt.model.export - INFO - Model successfully exported to /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_baseline_fp32.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline FP32 model exported successfully.\n",
      "ONNX Runtime (FP32 Baseline) output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load the baseline FP32 model\n",
    "print(f\"Loading baseline FP32 model from version: {baseline_fp32_version}\")\n",
    "baseline_fp32_model, _ = get_mobilenetv2_cifar10_model(\n",
    "    version=baseline_fp32_version\n",
    ")\n",
    "baseline_fp32_model.eval()\n",
    "baseline_fp32_model.to('cpu') # Move model to CPU for export\n",
    "\n",
    "# Define ONNX path\n",
    "onnx_path_baseline_fp32 = os.path.join(ONNX_EXPORT_DIR, \"mobilenetv2_cifar10_baseline_fp32.onnx\")\n",
    "\n",
    "# Export to ONNX\n",
    "print(f\"Exporting baseline FP32 model to {onnx_path_baseline_fp32}...\")\n",
    "success_fp32 = export_model_to_onnx(\n",
    "    model=baseline_fp32_model,\n",
    "    dummy_input=dummy_input,\n",
    "    onnx_path=onnx_path_baseline_fp32,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=13\n",
    ")\n",
    "\n",
    "if success_fp32:\n",
    "    print(\"Baseline FP32 model exported successfully.\")\n",
    "    # Run inference with ONNX Runtime\n",
    "    try:\n",
    "        ort_session_fp32 = ort.InferenceSession(onnx_path_baseline_fp32, providers=['CPUExecutionProvider'])\n",
    "        input_name_fp32 = ort_session_fp32.get_inputs()[0].name\n",
    "        output_name_fp32 = ort_session_fp32.get_outputs()[0].name\n",
    "        \n",
    "        ort_inputs_fp32 = {input_name_fp32: dummy_input.cpu().numpy()}\n",
    "        ort_outputs_fp32 = ort_session_fp32.run([output_name_fp32], ort_inputs_fp32)\n",
    "        print(f\"ONNX Runtime (FP32 Baseline) output shape: {ort_outputs_fp32[0].shape}\")\n",
    "        # print(f\"ONNX Runtime (FP32 Baseline) output sample: {ort_outputs_fp32[0][0,:5]}\") # Print first 5 logits\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ONNX Runtime for FP32 baseline model: {e}\")\n",
    "else:\n",
    "    print(\"Baseline FP32 model export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae62bd9",
   "metadata": {},
   "source": [
    "### Evaluate Baseline FP32 ONNX Model\n",
    "\n",
    "Now, let's use the `eval_onnx_model` function to evaluate the exported baseline FP32 ONNX model. We'll try this on both CPU and GPU (if available) to compare performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c372a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:15:19,754 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading existing training and validation datasets...\n",
      "2025-06-11 15:15:21,903 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading existing test dataset...\n",
      "2025-06-11 15:15:22,140 - nnopt.model.eval - INFO - Starting ONNX model evaluation for: /home/pbeuran/repos/nnopt/models/onnx_exports/mobilenetv2_cifar10_baseline_fp32.onnx\n",
      "2025-06-11 15:15:22,141 - nnopt.model.eval - INFO - Evaluation on PyTorch device: cpu, batch size: 32\n",
      "2025-06-11 15:15:22,141 - nnopt.model.eval - INFO - Using ONNX Runtime providers: ['CPUExecutionProvider']\n",
      "2025-06-11 15:15:22,163 - nnopt.model.eval - INFO - ONNX Model Input Name: input, Output Name: output\n",
      "2025-06-11 15:15:22,164 - nnopt.model.eval - INFO - Starting warmup for 2 batches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Baseline FP32 ONNX Model on CPU ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ONNX Warmup]: 100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n",
      "2025-06-11 15:15:22,805 - nnopt.model.eval - INFO - Warmup complete.\n",
      "2025-06-11 15:15:22,806 - nnopt.model.eval - INFO - Starting ONNX model evaluation pass...\n",
      "[ONNX Evaluation]: 100%|██████████| 157/157 [00:21<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Evaluation Complete: Avg Loss: 0.2994, Accuracy: 0.8970\n",
      "Throughput: 291.97 samples/sec | Avg Batch Time: 109.08 ms | Avg Sample Time: 3.43 ms\n",
      "System Stats (PyTorch side): CPU Usage: 81.00% | RAM Usage: 8.5/30.9GB (33.4%)\n",
      "CPU ONNX Metrics: {'accuracy': 0.897, 'avg_loss': 0.29936650104522705, 'samples_per_second': 291.9695883088593, 'avg_time_per_batch': 0.1090768868854669, 'avg_time_per_sample': 0.003425014248203661}\n",
      "\n",
      "Skipping GPU ONNX evaluation as CUDA is not available or ONNX Runtime GPU provider is not set up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions and data\n",
    "from nnopt.model.eval import eval_onnx_model\n",
    "from nnopt.recipes.mobilenetv2_cifar10 import get_cifar10_datasets\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "# Assuming the data is already downloaded and preprocessed as in other notebooks\n",
    "# Adjust data_dir if your CIFAR-10 data is located elsewhere\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'image', 'cifar10') \n",
    "_, test_dataset, _ = get_cifar10_datasets() # We only need test_dataset\n",
    "\n",
    "if success_fp32: # Only proceed if the ONNX model was exported successfully\n",
    "    print(\"\\n--- Evaluating Baseline FP32 ONNX Model on CPU ---\")\n",
    "    onnx_metrics_cpu = eval_onnx_model(\n",
    "        onnx_model_path=onnx_path_baseline_fp32,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=32, # Adjust as needed\n",
    "        device=\"cpu\",\n",
    "        num_warmup_batches=2 # Smaller warmup for quicker testing\n",
    "    )\n",
    "    print(f\"CPU ONNX Metrics: {onnx_metrics_cpu}\")\n",
    "\n",
    "    if torch.cuda.is_available() and ort.get_device() == 'GPU':\n",
    "        print(\"\\n--- Evaluating Baseline FP32 ONNX Model on GPU ---\")\n",
    "        onnx_metrics_gpu = eval_onnx_model(\n",
    "            onnx_model_path=onnx_path_baseline_fp32,\n",
    "            test_dataset=test_dataset,\n",
    "            batch_size=32, # Adjust as needed\n",
    "            device=\"cuda\",\n",
    "            num_warmup_batches=2\n",
    "        )\n",
    "        print(f\"GPU ONNX Metrics: {onnx_metrics_gpu}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping GPU ONNX evaluation as CUDA is not available or ONNX Runtime GPU provider is not set up.\")\n",
    "else:\n",
    "    print(\"\\nSkipping ONNX model evaluation as the export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4166a",
   "metadata": {},
   "source": [
    "## 2. Baseline QAT INT8 Model Export & Inference\n",
    "\n",
    "For QAT models, PyTorch's `torch.quantization.convert` (typically called at the end of QAT) prepares the model with quantized weights and operations. When loading a saved QAT model, it should already be in this converted state if saved correctly. The `get_mobilenetv2_cifar10_model` with `quantized=True` loads `torchvision.models.quantization.mobilenet_v2` which is suitable for QAT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c018d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:17:54,605 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loading MobileNetV2 model for CIFAR-10 (architecture and weights) from version: mobilenetv2_cifar10/int8/qat_mobilenetv2_cifar10/fp32/baseline at /home/pbeuran/repos/nnopt/models\n",
      "2025-06-11 15:17:54,658 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Successfully loaded model from /home/pbeuran/repos/nnopt/models/mobilenetv2_cifar10/int8/qat_mobilenetv2_cifar10/fp32/baseline/model.pt\n",
      "2025-06-11 15:17:54,659 - nnopt.recipes.mobilenetv2_cifar10 - INFO - Loaded metadata: {'metrics_values': {'accuracy': 0.8762, 'avg_loss': 0.36691694159507754, 'samples_per_second': 685.1813385203897, 'avg_time_per_batch': 0.046479861560056304, 'avg_time_per_sample': 0.001459467652985768, 'params_stats': {'int_weight_params': 2202560, 'float_weight_params': 0, 'float_bias_params': 17066, 'bn_param_params': 0, 'other_float_params': 0, 'total_params': 2219626, 'approx_memory_mb_for_params': 2.1656265258789062}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Baseline QAT INT8 model from version: mobilenetv2_cifar10/int8/qat_mobilenetv2_cifar10/fp32/baseline\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConvReLU2d' object has no attribute '_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# The QAT model is saved after torch.quantization.convert, so it's already an INT8 model.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# We use quantized=True in get_mobilenetv2_cifar10_model to load the correct model architecture\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# (e.g., torchvision.models.quantization.mobilenet_v2)\u001b[39;00m\n\u001b[32m      6\u001b[39m qat_int8_model, _ = get_mobilenetv2_cifar10_model(\n\u001b[32m      7\u001b[39m     version=qat_int8_version,\n\u001b[32m      8\u001b[39m     device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# Ensure model is loaded on CPU for export\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mqat_int8_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m qat_int8_model.to(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Quantized models run on CPU. ONNX export also expects CPU model.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Define ONNX path\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/nnopt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2874\u001b[39m, in \u001b[36mModule.eval\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2858\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meval\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) -> T:\n\u001b[32m   2859\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Set the module in evaluation mode.\u001b[39;00m\n\u001b[32m   2860\u001b[39m \n\u001b[32m   2861\u001b[39m \u001b[33;03m    This has an effect only on certain modules. See the documentation of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2872\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   2873\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2874\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/nnopt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2855\u001b[39m, in \u001b[36mModule.train\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m   2853\u001b[39m \u001b[38;5;28mself\u001b[39m.training = mode\n\u001b[32m   2854\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m-> \u001b[39m\u001b[32m2855\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/nnopt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2855\u001b[39m, in \u001b[36mModule.train\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m   2853\u001b[39m \u001b[38;5;28mself\u001b[39m.training = mode\n\u001b[32m   2854\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m-> \u001b[39m\u001b[32m2855\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/nnopt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2855\u001b[39m, in \u001b[36mModule.train\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m   2853\u001b[39m \u001b[38;5;28mself\u001b[39m.training = mode\n\u001b[32m   2854\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m-> \u001b[39m\u001b[32m2855\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/nnopt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2854\u001b[39m, in \u001b[36mModule.train\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m   2852\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtraining mode is expected to be boolean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2853\u001b[39m \u001b[38;5;28mself\u001b[39m.training = mode\n\u001b[32m-> \u001b[39m\u001b[32m2854\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/nnopt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2737\u001b[39m, in \u001b[36mModule.children\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   2732\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over immediate children modules.\u001b[39;00m\n\u001b[32m   2733\u001b[39m \n\u001b[32m   2734\u001b[39m \u001b[33;03m    Yields:\u001b[39;00m\n\u001b[32m   2735\u001b[39m \u001b[33;03m        Module: a child module\u001b[39;00m\n\u001b[32m   2736\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2737\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnamed_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/nnopt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2755\u001b[39m, in \u001b[36mModule.named_children\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2741\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\u001b[39;00m\n\u001b[32m   2742\u001b[39m \n\u001b[32m   2743\u001b[39m \u001b[33;03mYields:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2752\u001b[39m \n\u001b[32m   2753\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2754\u001b[39m memo = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m-> \u001b[39m\u001b[32m2755\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m.items():\n\u001b[32m   2756\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[32m   2757\u001b[39m         memo.add(module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/nnopt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'ConvReLU2d' object has no attribute '_modules'"
     ]
    }
   ],
   "source": [
    "# Load the Baseline QAT INT8 model\n",
    "print(f\"Loading Baseline QAT INT8 model from version: {qat_int8_version}\")\n",
    "# The QAT model is saved after torch.quantization.convert, so it's already an INT8 model.\n",
    "# We use quantized=True in get_mobilenetv2_cifar10_model to load the correct model architecture\n",
    "# (e.g., torchvision.models.quantization.mobilenet_v2)\n",
    "qat_int8_model, _ = get_mobilenetv2_cifar10_model(\n",
    "    version=qat_int8_version,\n",
    "    device='cpu' # Ensure model is loaded on CPU for export\n",
    ")\n",
    "qat_int8_model.eval()\n",
    "qat_int8_model.to('cpu') # Quantized models run on CPU. ONNX export also expects CPU model.\n",
    "\n",
    "# Define ONNX path\n",
    "onnx_path_qat_int8 = os.path.join(ONNX_EXPORT_DIR, \"mobilenetv2_cifar10_qat_int8.onnx\")\n",
    "\n",
    "# Export to ONNX\n",
    "# Opset version 13+ is generally recommended for better support of quantized operators.\n",
    "print(f\"Exporting Baseline QAT INT8 model to {onnx_path_qat_int8}...\")\n",
    "success_qat_int8 = export_model_to_onnx(\n",
    "    model=qat_int8_model,\n",
    "    dummy_input=dummy_input, # Dummy input should be FP32 for QAT model export\n",
    "    onnx_path=onnx_path_qat_int8,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=13 # Use opset 13 or higher for QAT models\n",
    ")\n",
    "\n",
    "if success_qat_int8:\n",
    "    print(\"Baseline QAT INT8 model exported successfully.\")\n",
    "    # Run inference with ONNX Runtime\n",
    "    try:\n",
    "        ort_session_qat_int8 = ort.InferenceSession(onnx_path_qat_int8, providers=['CPUExecutionProvider'])\n",
    "        input_name_qat_int8 = ort_session_qat_int8.get_inputs()[0].name\n",
    "        output_name_qat_int8 = ort_session_qat_int8.get_outputs()[0].name\n",
    "        \n",
    "        # Input to ONNX Runtime for QAT model is also FP32\n",
    "        ort_inputs_qat_int8 = {input_name_qat_int8: dummy_input.cpu().numpy()}\n",
    "        ort_outputs_qat_int8 = ort_session_qat_int8.run([output_name_qat_int8], ort_inputs_qat_int8)\n",
    "        print(f\"ONNX Runtime (QAT INT8) output shape: {ort_outputs_qat_int8[0].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ONNX Runtime for QAT INT8 model: {e}\")\n",
    "else:\n",
    "    print(\"Baseline QAT INT8 model export failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3a84b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mobilenetv2_cifar10/int8/qat_mobilenetv2_cifar10/fp32/baseline'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qat_int8_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2c29c",
   "metadata": {},
   "source": [
    "### Evaluate Baseline QAT INT8 ONNX Model\n",
    "\n",
    "Finally, we evaluate the exported baseline QAT INT8 ONNX model using `eval_onnx_model`. \n",
    "Note: QAT INT8 models are typically run on CPU. While ONNX Runtime supports some quantized operators on GPU via TensorRT or CUDA execution providers, CPU is the more common target for these models. We will primarily test on CPU here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935838e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure test_dataset is loaded\n",
    "\n",
    "if success_qat_int8: # Only proceed if the ONNX model was exported successfully\n",
    "    print(\"\\n--- Evaluating Baseline QAT INT8 ONNX Model on CPU ---\")\n",
    "    onnx_metrics_qat_int8_cpu = eval_onnx_model(\n",
    "        onnx_model_path=onnx_path_qat_int8,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=32, \n",
    "        device=\"cpu\", # QAT models are typically evaluated on CPU\n",
    "        num_warmup_batches=2\n",
    "    )\n",
    "    print(f\"CPU ONNX Metrics (QAT INT8): {onnx_metrics_qat_int8_cpu}\")\n",
    "\n",
    "    # Optional: Test QAT INT8 on GPU if supported and desired\n",
    "    # Note: GPU support for INT8 ONNX models can be more complex and might require specific ONNX opset versions\n",
    "    # or specific GPU capabilities and ONNX Runtime build options.\n",
    "    if torch.cuda.is_available() and ort.get_device() == 'GPU':\n",
    "        print(\"\\n--- Evaluating Baseline QAT INT8 ONNX Model on GPU (Experimental) ---\")\n",
    "        try:\n",
    "            onnx_metrics_qat_int8_gpu = eval_onnx_model(\n",
    "                onnx_model_path=onnx_path_qat_int8,\n",
    "                test_dataset=test_dataset,\n",
    "                batch_size=32, \n",
    "                device=\"cuda\",\n",
    "                num_warmup_batches=2\n",
    "            )\n",
    "            print(f\"GPU ONNX Metrics (QAT INT8): {onnx_metrics_qat_int8_gpu}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not run QAT INT8 ONNX model on GPU: {e}\")\n",
    "            print(\"This might be due to operator support or other configuration issues.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping GPU ONNX evaluation for QAT INT8 model as CUDA is not available or ONNX Runtime GPU provider is not set up.\")\n",
    "else:\n",
    "    print(\"\\nSkipping ONNX model evaluation for QAT INT8 model as the export failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a88514",
   "metadata": {},
   "source": [
    "With these steps, the three specified models are exported to ONNX format and tested with ONNX Runtime. These ONNX models can now be used for deployment or further conversion to other formats like TensorRT (for NVIDIA GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853244b2",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d45291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from nnopt.model.eval import eval_model # For PyTorch model evaluation\n",
    "from nnopt.recipes.mobilenetv2_cifar10 import get_mobilenetv2_cifar10_model, get_cifar10_datasets # To load models and dataset\n",
    "\n",
    "# Ensure test_dataset is loaded (it should be from earlier cells, e.g., cell d52e6d98)\n",
    "# If not, uncomment and run:\n",
    "# DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'image', 'cifar10')\n",
    "# _, _, test_dataset = get_cifar10_datasets(data_dir=DATA_DIR) # Or however you load it\n",
    "\n",
    "# --- 1. PyTorch FP32 Baseline Model Evaluation (CPU) ---\n",
    "print(\"Evaluating PyTorch FP32 Baseline Model on CPU for analysis...\")\n",
    "pytorch_fp32_model, _ = get_mobilenetv2_cifar10_model(\n",
    "    version=baseline_fp32_version, # Defined in cell f411f025\n",
    "    device='cpu'\n",
    ")\n",
    "pytorch_fp32_model.eval()\n",
    "pytorch_fp32_metrics_cpu = eval_model(\n",
    "    model=pytorch_fp32_model,\n",
    "    test_dataset=test_dataset,\n",
    "    device=\"cpu\",\n",
    "    use_amp=False, # No AMP for CPU FP32\n",
    "    dtype=torch.float32,\n",
    "    batch_size=32, # Consistent batch size\n",
    "    num_warmup_batches=2\n",
    ")\n",
    "print(f\"PyTorch FP32 CPU Metrics: {pytorch_fp32_metrics_cpu}\")\n",
    "\n",
    "# --- 2. PyTorch QAT INT8 Baseline Model Evaluation (CPU) ---\n",
    "print(\"\\nEvaluating PyTorch QAT INT8 Baseline Model on CPU for analysis...\")\n",
    "# qat_int8_version is defined in cell f411f025\n",
    "pytorch_qat_int8_model, _ = get_mobilenetv2_cifar10_model(\n",
    "    version=qat_int8_version,\n",
    "    device='cpu'\n",
    ")\n",
    "pytorch_qat_int8_model.eval()\n",
    "pytorch_qat_int8_metrics_cpu = eval_model(\n",
    "    model=pytorch_qat_int8_model,\n",
    "    test_dataset=test_dataset,\n",
    "    device=\"cpu\",\n",
    "    use_amp=False, # QAT models are typically run with FP32 interface, actual ops are INT8\n",
    "    dtype=torch.float32,\n",
    "    batch_size=32,\n",
    "    num_warmup_batches=2\n",
    ")\n",
    "print(f\"PyTorch QAT INT8 CPU Metrics: {pytorch_qat_int8_metrics_cpu}\")\n",
    "\n",
    "# --- 3. Retrieve ONNX Model Metrics (Assumed to be available from previous cells) ---\n",
    "# Ensure 'onnx_metrics_cpu' and 'onnx_metrics_qat_int8_cpu' are populated from earlier cells\n",
    "# These should contain keys like 'accuracy', 'avg_time_per_sample'\n",
    "print(f\"\\nUsing pre-calculated ONNX FP32 CPU Metrics: {onnx_metrics_cpu}\")\n",
    "print(f\"Using pre-calculated ONNX QAT INT8 CPU Metrics: {onnx_metrics_qat_int8_cpu}\")\n",
    "\n",
    "\n",
    "# --- 4. Model Sizes ---\n",
    "# PyTorch model sizes (from parameters, does not include quantization overhead directly but reflects param precision)\n",
    "# For a more direct comparison with ONNX file size, you could save the PyTorch models and get file size.\n",
    "# Here, we use the parameter-based approximation for PyTorch models.\n",
    "pytorch_fp32_size_mb = pytorch_fp32_metrics_cpu['params_stats']['total_params'] * pytorch_fp32_metrics_cpu['params_stats']['approx_memory_mb_for_params'] / pytorch_fp32_metrics_cpu['params_stats']['total_params'] if pytorch_fp32_metrics_cpu['params_stats']['total_params'] > 0 else 0\n",
    "# For QAT INT8, the parameters are still stored in FP32 for training, but effective size is smaller.\n",
    "# The 'params_stats' from eval_model for a quantized model might still reflect FP32 storage if not careful.\n",
    "# A better measure for PyTorch quantized model size is to save it and check file size, or estimate based on INT8.\n",
    "# For simplicity, we'll use the reported size from eval_model, but acknowledge it might be an overestimate for QAT.\n",
    "# A more accurate way for PyTorch INT8 model size:\n",
    "torch.save(pytorch_qat_int8_model.state_dict(), \"temp_qat_int8_model.pth\")\n",
    "pytorch_qat_int8_size_mb = os.path.getsize(\"temp_qat_int8_model.pth\") / (1024 * 1024)\n",
    "os.remove(\"temp_qat_int8_model.pth\")\n",
    "print(f\"PyTorch QAT INT8 Model Size (saved state_dict): {pytorch_qat_int8_size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "# ONNX model file sizes\n",
    "# Ensure 'onnx_path_baseline_fp32' and 'onnx_path_qat_int8' are defined (cell f411f025 and 5b5fb4cd, 2170bb29)\n",
    "onnx_fp32_size_mb = os.path.getsize(onnx_path_baseline_fp32) / (1024 * 1024) if os.path.exists(onnx_path_baseline_fp32) else 0\n",
    "onnx_qat_int8_size_mb = os.path.getsize(onnx_path_qat_int8) / (1024 * 1024) if os.path.exists(onnx_path_qat_int8) else 0\n",
    "print(f\"ONNX FP32 Model Size: {onnx_fp32_size_mb:.2f} MB\")\n",
    "print(f\"ONNX QAT INT8 Model Size: {onnx_qat_int8_size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "# --- 5. Prepare data for plotting ---\n",
    "model_labels = [\n",
    "    \"PyTorch FP32\",\n",
    "    \"PyTorch QAT INT8\",\n",
    "    \"ONNX FP32\",\n",
    "    \"ONNX QAT INT8\"\n",
    "]\n",
    "\n",
    "# Using test accuracies\n",
    "accuracies = [\n",
    "    pytorch_fp32_metrics_cpu['accuracy'],\n",
    "    pytorch_qat_int8_metrics_cpu['accuracy'],\n",
    "    onnx_metrics_cpu['accuracy'] if 'onnx_metrics_cpu' in locals() and onnx_metrics_cpu else 0, # from cell d52e6d98\n",
    "    onnx_metrics_qat_int8_cpu['accuracy'] if 'onnx_metrics_qat_int8_cpu' in locals() and onnx_metrics_qat_int8_cpu else 0 # from cell 8b57290a\n",
    "]\n",
    "\n",
    "# CPU inference time per sample (test set)\n",
    "cpu_time_per_sample = [\n",
    "    pytorch_fp32_metrics_cpu['avg_time_per_sample'],\n",
    "    pytorch_qat_int8_metrics_cpu['avg_time_per_sample'],\n",
    "    onnx_metrics_cpu['avg_time_per_sample'] if 'onnx_metrics_cpu' in locals() and onnx_metrics_cpu else float('inf'),\n",
    "    onnx_metrics_qat_int8_cpu['avg_time_per_sample'] if 'onnx_metrics_qat_int8_cpu' in locals() and onnx_metrics_qat_int8_cpu else float('inf')\n",
    "]\n",
    "\n",
    "# Model sizes in MB\n",
    "model_sizes_mb = [\n",
    "    pytorch_fp32_size_mb,\n",
    "    pytorch_qat_int8_size_mb, # Using saved state_dict size\n",
    "    onnx_fp32_size_mb,\n",
    "    onnx_qat_int8_size_mb\n",
    "]\n",
    "\n",
    "print(\"\\nData for plotting:\")\n",
    "print(f\"Labels: {model_labels}\")\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "print(f\"CPU Time/Sample (s): {cpu_time_per_sample}\")\n",
    "print(f\"Model Sizes (MB): {model_sizes_mb}\")\n",
    "\n",
    "# Check if all ONNX metrics were loaded correctly\n",
    "if not ('onnx_metrics_cpu' in locals() and onnx_metrics_cpu and \\\n",
    "        'onnx_metrics_qat_int8_cpu' in locals() and onnx_metrics_qat_int8_cpu):\n",
    "    print(\"\\nWARNING: ONNX metrics might not be fully loaded. Plots might be incomplete or show zero/infinity values.\")\n",
    "    print(\"Please ensure the cells evaluating ONNX models (d52e6d98, 8b57290a) have been run successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Bar Plot (Test Set on CPU)\n",
    "x = np.arange(len(model_labels))\n",
    "width = 0.5 # Single bar for test accuracy\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects = ax.bar(x, accuracies, width, label='Test Accuracy (CPU)')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Test Accuracy Comparison (CPU)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.set_ylim(min(accuracies) * 0.9 if min(accuracies) > 0 else 0, max(accuracies) * 1.1 if max(accuracies) > 0 else 1) # Adjust y-lim dynamically\n",
    "\n",
    "def autolabel(rects_to_label, ax_to_use):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects_to_label:\n",
    "        height = rect.get_height()\n",
    "        ax_to_use.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "autolabel(rects, ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8702fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Inference Time Comparison Plot (Time per Sample)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Convert times to milliseconds for better readability if they are very small\n",
    "cpu_time_per_sample_ms = [t * 1000 for t in cpu_time_per_sample]\n",
    "rects = ax.bar(x, cpu_time_per_sample_ms, width, label='CPU Time/Sample (ms)')\n",
    "\n",
    "ax.set_ylabel('CPU Time/Sample (milliseconds)')\n",
    "ax.set_title('Model Inference Time Comparison (CPU)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "# ax.set_yscale('log') # Use log scale if times vary greatly\n",
    "\n",
    "def autolabel_time(rects_to_label, ax_to_use):\n",
    "    for rect in rects_to_label:\n",
    "        height = rect.get_height()\n",
    "        ax_to_use.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "autolabel_time(rects, ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Size Comparison Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects = ax.bar(x, model_sizes_mb, width, label='Model Size (MB)')\n",
    "\n",
    "ax.set_ylabel('Model Size (MB)')\n",
    "ax.set_title('Model Size Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "def autolabel_size(rects_to_label, ax_to_use):\n",
    "    for rect in rects_to_label:\n",
    "        height = rect.get_height()\n",
    "        ax_to_use.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "autolabel_size(rects, ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
